#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æœ¬åœ°æ•°æ®åº“ç»“æ„åŒæ­¥åˆ°äº‘ç«¯è„šæœ¬
åŠŸèƒ½ï¼šå°†æœ¬åœ°æ•°æ®åº“ç»“æ„å®‰å…¨åœ°åŒæ­¥åˆ°äº‘ç«¯ï¼Œä¸ç ´åä»»ä½•ç°æœ‰æ•°æ®
"""

import os
import sys
import psycopg2
import logging
import subprocess
import datetime
from urllib.parse import urlparse

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger('ç»“æ„åŒæ­¥')

class SchemaSync:
    def __init__(self):
        self.timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')
        self.sync_dir = os.path.join(os.getcwd(), 'sync_logs')
        os.makedirs(self.sync_dir, exist_ok=True)
        
        # äº‘ç«¯æ•°æ®åº“è¿æ¥ä¿¡æ¯
        self.cloud_db_url = "postgresql://pma_db_sp8d_user:LXNGJmR6bFrNecoaWbdbdzPpltIAd40w@dpg-d0b1gl1r0fns73d1jc1g-a.singapore-postgres.render.com/pma_db_sp8d"
        
        # æœ¬åœ°æ•°æ®åº“è¿æ¥ä¿¡æ¯
        self.local_db_url = "postgresql://nijie@localhost:5432/pma_local"
        
        logger.info(f"æœ¬åœ°æ•°æ®åº“: {self.local_db_url.split('@')[1]}")
        logger.info(f"äº‘ç«¯æ•°æ®åº“: {self.cloud_db_url.split('@')[1]}")

    def parse_db_url(self, db_url):
        """è§£ææ•°æ®åº“URL"""
        parsed = urlparse(db_url)
        return {
            'host': parsed.hostname,
            'port': parsed.port or 5432,
            'user': parsed.username,
            'password': parsed.password,
            'dbname': parsed.path.lstrip('/')
        }

    def test_connections(self):
        """æµ‹è¯•ä¸¤ä¸ªæ•°æ®åº“è¿æ¥"""
        logger.info("æµ‹è¯•æ•°æ®åº“è¿æ¥...")
        
        # æµ‹è¯•æœ¬åœ°è¿æ¥
        try:
            local_params = self.parse_db_url(self.local_db_url)
            conn = psycopg2.connect(**local_params)
            cursor = conn.cursor()
            cursor.execute("SELECT COUNT(*) FROM information_schema.tables WHERE table_schema = 'public'")
            local_tables = cursor.fetchone()[0]
            logger.info(f"âœ… æœ¬åœ°æ•°æ®åº“è¿æ¥æˆåŠŸï¼Œå…± {local_tables} ä¸ªè¡¨")
            cursor.close()
            conn.close()
        except Exception as e:
            logger.error(f"âŒ æœ¬åœ°æ•°æ®åº“è¿æ¥å¤±è´¥: {e}")
            return False
        
        # æµ‹è¯•äº‘ç«¯è¿æ¥
        try:
            cloud_params = self.parse_db_url(self.cloud_db_url)
            conn = psycopg2.connect(**cloud_params)
            cursor = conn.cursor()
            cursor.execute("SELECT COUNT(*) FROM information_schema.tables WHERE table_schema = 'public'")
            cloud_tables = cursor.fetchone()[0]
            logger.info(f"âœ… äº‘ç«¯æ•°æ®åº“è¿æ¥æˆåŠŸï¼Œå…± {cloud_tables} ä¸ªè¡¨")
            cursor.close()
            conn.close()
        except Exception as e:
            logger.error(f"âŒ äº‘ç«¯æ•°æ®åº“è¿æ¥å¤±è´¥: {e}")
            return False
        
        return True

    def get_local_schema(self):
        """å¯¼å‡ºæœ¬åœ°æ•°æ®åº“ç»“æ„"""
        logger.info("å¯¼å‡ºæœ¬åœ°æ•°æ®åº“ç»“æ„...")
        
        local_params = self.parse_db_url(self.local_db_url)
        schema_file = os.path.join(self.sync_dir, f'local_schema_{self.timestamp}.sql')
        
        try:
            cmd = [
                'pg_dump',
                '-h', local_params['host'],
                '-p', str(local_params['port']),
                '-U', local_params['user'],
                '-d', local_params['dbname'],
                '--schema-only',
                '--no-owner',
                '--no-privileges',
                '--clean',
                '--if-exists',
                '-f', schema_file
            ]
            
            env = os.environ.copy()
            if local_params.get('password'):
                env['PGPASSWORD'] = local_params['password']
            
            result = subprocess.run(cmd, env=env, capture_output=True, text=True)
            
            if result.returncode == 0:
                logger.info(f"âœ… æœ¬åœ°ç»“æ„å¯¼å‡ºæˆåŠŸ: {schema_file}")
                return schema_file
            else:
                logger.error(f"âŒ æœ¬åœ°ç»“æ„å¯¼å‡ºå¤±è´¥: {result.stderr}")
                return None
                
        except Exception as e:
            logger.error(f"âŒ å¯¼å‡ºè¿‡ç¨‹å‡ºé”™: {e}")
            return None

    def make_schema_safe(self, schema_sql):
        """å¤„ç†schema SQLï¼Œä½¿å…¶å®‰å…¨ï¼ˆä¸ç ´åç°æœ‰æ•°æ®ï¼‰"""
        logger.info("å¤„ç†SQLï¼Œç¡®ä¿å®‰å…¨åŒæ­¥...")
        
        lines = schema_sql.split('\n')
        safe_lines = []
        dropped_statements = []
        
        for line in lines:
            line_stripped = line.strip()
            line_lower = line_stripped.lower()
            
            # è·³è¿‡å±é™©è¯­å¥ä½†è®°å½•
            if (line_lower.startswith('drop ') and 
                ('table' in line_lower or 'database' in line_lower or 'schema' in line_lower)):
                dropped_statements.append(line_stripped)
                logger.debug(f"è·³è¿‡DROPè¯­å¥: {line_stripped}")
                continue
            
            # è·³è¿‡å…¶ä»–å±é™©æ“ä½œ
            if (line_lower.startswith('delete ') or 
                line_lower.startswith('truncate ') or
                line_lower.startswith('alter table') and 'drop column' in line_lower):
                dropped_statements.append(line_stripped)
                logger.debug(f"è·³è¿‡å±é™©è¯­å¥: {line_stripped}")
                continue
            
            # å°†CREATE TABLEæ”¹ä¸ºCREATE TABLE IF NOT EXISTS
            if line_lower.startswith('create table '):
                line = line.replace('CREATE TABLE ', 'CREATE TABLE IF NOT EXISTS ')
                line = line.replace('create table ', 'CREATE TABLE IF NOT EXISTS ')
                logger.debug(f"ä¿®æ”¹ä¸ºå®‰å…¨åˆ›å»º: {line.strip()}")
            
            # å°†CREATE INDEXæ”¹ä¸ºCREATE INDEX IF NOT EXISTS
            if line_lower.startswith('create index ') or line_lower.startswith('create unique index '):
                if 'if not exists' not in line_lower:
                    if line_lower.startswith('create unique index '):
                        line = line.replace('CREATE UNIQUE INDEX ', 'CREATE UNIQUE INDEX IF NOT EXISTS ')
                        line = line.replace('create unique index ', 'CREATE UNIQUE INDEX IF NOT EXISTS ')
                    else:
                        line = line.replace('CREATE INDEX ', 'CREATE INDEX IF NOT EXISTS ')
                        line = line.replace('create index ', 'CREATE INDEX IF NOT EXISTS ')
            
            safe_lines.append(line)
        
        logger.info(f"å·²è¿‡æ»¤ {len(dropped_statements)} ä¸ªå±é™©è¯­å¥")
        for stmt in dropped_statements[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
            logger.info(f"  - {stmt}")
        if len(dropped_statements) > 5:
            logger.info(f"  ... è¿˜æœ‰ {len(dropped_statements) - 5} ä¸ª")
        
        return '\n'.join(safe_lines)

    def backup_cloud_data_before_sync(self):
        """åŒæ­¥å‰å†æ¬¡ç¡®è®¤äº‘ç«¯æ•°æ®å®Œæ•´æ€§"""
        logger.info("åŒæ­¥å‰éªŒè¯äº‘ç«¯æ•°æ®å®Œæ•´æ€§...")
        
        try:
            cloud_params = self.parse_db_url(self.cloud_db_url)
            conn = psycopg2.connect(**cloud_params)
            cursor = conn.cursor()
            
            # è·å–è¡¨è¡Œæ•°ç»Ÿè®¡
            cursor.execute("""
                SELECT table_name
                FROM information_schema.tables 
                WHERE table_schema = 'public' AND table_type = 'BASE TABLE'
                ORDER BY table_name;
            """)
            tables = cursor.fetchall()
            
            total_rows = 0
            table_data = {}
            
            for (table_name,) in tables:
                try:
                    cursor.execute(f"SELECT COUNT(*) FROM {table_name};")
                    count = cursor.fetchone()[0]
                    table_data[table_name] = count
                    total_rows += count
                except Exception as e:
                    logger.warning(f"æ— æ³•è·å–è¡¨ {table_name} çš„è¡Œæ•°: {e}")
                    table_data[table_name] = 0
            
            logger.info(f"äº‘ç«¯å½“å‰æ€»æ•°æ®è¡Œæ•°: {total_rows}")
            
            cursor.close()
            conn.close()
            
            return total_rows, table_data
            
        except Exception as e:
            logger.error(f"äº‘ç«¯æ•°æ®éªŒè¯å¤±è´¥: {e}")
            return 0, {}

    def sync_schema_safely(self, schema_file):
        """å®‰å…¨åœ°åŒæ­¥ç»“æ„åˆ°äº‘ç«¯"""
        logger.info("å¼€å§‹å®‰å…¨åŒæ­¥ç»“æ„åˆ°äº‘ç«¯...")
        
        # è¯»å–å¹¶å¤„ç†ç»“æ„æ–‡ä»¶
        try:
            with open(schema_file, 'r', encoding='utf-8') as f:
                schema_sql = f.read()
        except Exception as e:
            logger.error(f"è¯»å–ç»“æ„æ–‡ä»¶å¤±è´¥: {e}")
            return False
        
        # å¤„ç†ä¸ºå®‰å…¨SQL
        safe_sql = self.make_schema_safe(schema_sql)
        
        # ä¿å­˜å®‰å…¨SQLåˆ°æ–‡ä»¶
        safe_schema_file = os.path.join(self.sync_dir, f'safe_schema_{self.timestamp}.sql')
        with open(safe_schema_file, 'w', encoding='utf-8') as f:
            f.write(safe_sql)
        logger.info(f"å®‰å…¨SQLå·²ä¿å­˜: {safe_schema_file}")
        
        # è¿æ¥äº‘ç«¯æ•°æ®åº“æ‰§è¡ŒåŒæ­¥
        cloud_params = self.parse_db_url(self.cloud_db_url)
        
        try:
            conn = psycopg2.connect(**cloud_params)
            conn.autocommit = False
            cursor = conn.cursor()
            
            try:
                logger.info("æ‰§è¡Œå®‰å…¨ç»“æ„åŒæ­¥...")
                cursor.execute(safe_sql)
                conn.commit()
                logger.info("âœ… ç»“æ„åŒæ­¥æˆåŠŸ")
                
                # éªŒè¯åŒæ­¥ç»“æœ
                cursor.execute("""
                    SELECT table_name FROM information_schema.tables 
                    WHERE table_schema = 'public' 
                    ORDER BY table_name;
                """)
                tables_after = [t[0] for t in cursor.fetchall()]
                logger.info(f"åŒæ­¥åè¡¨æ•°é‡: {len(tables_after)}")
                
                return True
                
            except Exception as e:
                logger.error(f"âŒ ç»“æ„åŒæ­¥å¤±è´¥: {e}")
                conn.rollback()
                return False
            finally:
                cursor.close()
                conn.close()
                
        except Exception as e:
            logger.error(f"âŒ è¿æ¥äº‘ç«¯æ•°æ®åº“å¤±è´¥: {e}")
            return False

    def verify_data_after_sync(self, before_data):
        """åŒæ­¥åéªŒè¯æ•°æ®å®Œæ•´æ€§"""
        logger.info("éªŒè¯åŒæ­¥åæ•°æ®å®Œæ•´æ€§...")
        
        try:
            cloud_params = self.parse_db_url(self.cloud_db_url)
            conn = psycopg2.connect(**cloud_params)
            cursor = conn.cursor()
            
            # è·å–åŒæ­¥åçš„è¡¨è¡Œæ•°ç»Ÿè®¡
            cursor.execute("""
                SELECT table_name
                FROM information_schema.tables 
                WHERE table_schema = 'public' AND table_type = 'BASE TABLE'
                ORDER BY table_name;
            """)
            tables = cursor.fetchall()
            
            total_rows_after = 0
            table_data_after = {}
            
            for (table_name,) in tables:
                try:
                    cursor.execute(f"SELECT COUNT(*) FROM {table_name};")
                    count = cursor.fetchone()[0]
                    table_data_after[table_name] = count
                    total_rows_after += count
                except Exception as e:
                    logger.warning(f"æ— æ³•è·å–è¡¨ {table_name} çš„è¡Œæ•°: {e}")
                    table_data_after[table_name] = 0
            
            cursor.close()
            conn.close()
            
            # å¯¹æ¯”æ•°æ®
            before_total = sum(before_data.values())
            logger.info(f"åŒæ­¥å‰æ€»è¡Œæ•°: {before_total}")
            logger.info(f"åŒæ­¥åæ€»è¡Œæ•°: {total_rows_after}")
            
            if before_total == total_rows_after:
                logger.info("âœ… æ•°æ®å®Œæ•´æ€§éªŒè¯é€šè¿‡ï¼Œæ— æ•°æ®ä¸¢å¤±")
                return True
            else:
                logger.warning(f"âš ï¸ æ•°æ®è¡Œæ•°å˜åŒ–: {total_rows_after - before_total}")
                
                # æ£€æŸ¥å…·ä½“å“ªäº›è¡¨æœ‰å˜åŒ–
                for table, before_count in before_data.items():
                    after_count = table_data_after.get(table, 0)
                    if before_count != after_count:
                        logger.warning(f"  è¡¨ {table}: {before_count} -> {after_count}")
                
                return False
            
        except Exception as e:
            logger.error(f"æ•°æ®éªŒè¯å¤±è´¥: {e}")
            return False

    def generate_sync_report(self, schema_file, sync_success, data_verified, before_total, after_total):
        """ç”ŸæˆåŒæ­¥æŠ¥å‘Š"""
        report_file = os.path.join(self.sync_dir, f'sync_report_{self.timestamp}.md')
        
        status = "âœ… æˆåŠŸ" if sync_success and data_verified else "âŒ å¤±è´¥"
        data_status = "âœ… æ— å˜åŒ–" if before_total == after_total else f"âš ï¸ å˜åŒ– {after_total - before_total} è¡Œ"
        
        report_content = f"""# æ•°æ®åº“ç»“æ„åŒæ­¥æŠ¥å‘Š

## åŒæ­¥æ¦‚è¿°
- åŒæ­¥æ—¶é—´: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
- æœ¬åœ°æ•°æ®åº“: pma_local (localhost)
- äº‘ç«¯æ•°æ®åº“: pma_db_sp8d (Singapore Render)
- åŒæ­¥çŠ¶æ€: {status}
- æ•°æ®å®Œæ•´æ€§: {data_status}

## æ‰§è¡Œæ­¥éª¤
1. âœ… æµ‹è¯•æ•°æ®åº“è¿æ¥
2. {'âœ…' if schema_file else 'âŒ'} å¯¼å‡ºæœ¬åœ°æ•°æ®åº“ç»“æ„
3. âœ… å¤‡ä»½å‰æ•°æ®éªŒè¯
4. {'âœ…' if sync_success else 'âŒ'} å®‰å…¨ç»“æ„åŒæ­¥
5. {'âœ…' if data_verified else 'âŒ'} åŒæ­¥åæ•°æ®éªŒè¯

## æ•°æ®ç»Ÿè®¡
- åŒæ­¥å‰æ•°æ®è¡Œæ•°: {before_total}
- åŒæ­¥åæ•°æ®è¡Œæ•°: {after_total}
- æ•°æ®å˜åŒ–: {after_total - before_total} è¡Œ

## å®‰å…¨æªæ–½
- âœ… è·³è¿‡æ‰€æœ‰DROPè¯­å¥
- âœ… ä½¿ç”¨IF NOT EXISTSåˆ›å»ºè¡¨å’Œç´¢å¼•
- âœ… äº‹åŠ¡å›æ»šæœºåˆ¶
- âœ… æ•°æ®å®Œæ•´æ€§éªŒè¯

## æ–‡ä»¶ä½ç½®
- åŒæ­¥æ—¥å¿—ç›®å½•: {self.sync_dir}
- åŸå§‹ç»“æ„æ–‡ä»¶: {schema_file or 'æ— '}
- å®‰å…¨ç»“æ„æ–‡ä»¶: safe_schema_{self.timestamp}.sql

## ç»“è®º
{'âœ… ç»“æ„åŒæ­¥æˆåŠŸï¼Œæ•°æ®å®Œæ•´æ€§è‰¯å¥½' if sync_success and data_verified else 'âŒ åŒæ­¥è¿‡ç¨‹ä¸­å‡ºç°é—®é¢˜ï¼Œè¯·æ£€æŸ¥æ—¥å¿—'}

## å»ºè®®
1. æµ‹è¯•åº”ç”¨åŠŸèƒ½æ˜¯å¦æ­£å¸¸
2. æ£€æŸ¥æ–°å¢çš„è¡¨å’Œå­—æ®µ
3. éªŒè¯å…³é”®ä¸šåŠ¡æµç¨‹
4. å¦‚æœ‰é—®é¢˜ï¼Œå¯ä½¿ç”¨å¤‡ä»½æ–‡ä»¶æ¢å¤
"""
        
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report_content)
        
        logger.info(f"åŒæ­¥æŠ¥å‘Šå·²ç”Ÿæˆ: {report_file}")
        return report_file

    def run(self):
        """æ‰§è¡Œå®Œæ•´çš„ç»“æ„åŒæ­¥æµç¨‹"""
        logger.info("ğŸš€ å¼€å§‹æ•°æ®åº“ç»“æ„åŒæ­¥æµç¨‹...")
        
        # 1. æµ‹è¯•è¿æ¥
        if not self.test_connections():
            logger.error("âŒ æ•°æ®åº“è¿æ¥å¤±è´¥ï¼Œä¸­æ­¢åŒæ­¥")
            return False
        
        # 2. å¤‡ä»½å‰æ•°æ®éªŒè¯
        before_total, before_data = self.backup_cloud_data_before_sync()
        if before_total == 0:
            logger.error("âŒ äº‘ç«¯æ•°æ®éªŒè¯å¤±è´¥ï¼Œä¸­æ­¢åŒæ­¥")
            return False
        
        # 3. å¯¼å‡ºæœ¬åœ°ç»“æ„
        schema_file = self.get_local_schema()
        if not schema_file:
            logger.error("âŒ æœ¬åœ°ç»“æ„å¯¼å‡ºå¤±è´¥ï¼Œä¸­æ­¢åŒæ­¥")
            return False
        
        # 4. å®‰å…¨åŒæ­¥ç»“æ„
        sync_success = self.sync_schema_safely(schema_file)
        
        # 5. éªŒè¯æ•°æ®å®Œæ•´æ€§
        data_verified = self.verify_data_after_sync(before_data)
        
        # 6. è·å–åŒæ­¥åæ•°æ®ç»Ÿè®¡
        after_total, _ = self.backup_cloud_data_before_sync()
        
        # 7. ç”ŸæˆæŠ¥å‘Š
        report_file = self.generate_sync_report(schema_file, sync_success, data_verified, before_total, after_total)
        
        if sync_success and data_verified:
            logger.info("ğŸ‰ ç»“æ„åŒæ­¥æˆåŠŸå®Œæˆ!")
            logger.info(f"ğŸ“Š æ•°æ®ç»Ÿè®¡: {before_total} -> {after_total} è¡Œ")
            logger.info(f"ğŸ“‹ è¯¦ç»†æŠ¥å‘Š: {report_file}")
        else:
            logger.error("âŒ åŒæ­¥è¿‡ç¨‹ä¸­å‡ºç°é—®é¢˜ï¼Œè¯·æŸ¥çœ‹æŠ¥å‘Š")
        
        return sync_success and data_verified

if __name__ == "__main__":
    sync_tool = SchemaSync()
    success = sync_tool.run()
    if success:
        logger.info("ğŸ¯ ç»“æ„åŒæ­¥æ“ä½œæˆåŠŸå®Œæˆ!")
    else:
        logger.error("ğŸ’¥ ç»“æ„åŒæ­¥æ“ä½œå¤±è´¥!")
