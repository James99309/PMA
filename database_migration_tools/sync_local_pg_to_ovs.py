#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æœ¬åœ°PostgreSQLæ•°æ®åº“ç»“æ„åŒæ­¥åˆ°äº‘ç«¯pma_db_ovs
åŠŸèƒ½ï¼š
1. å¯¹æ¯”æœ¬åœ°PostgreSQL pma_localå’Œäº‘ç«¯pma_db_ovsçš„æ•°æ®åº“ç»“æ„
2. å°†æœ¬åœ°ç»“æ„åŒæ­¥åˆ°äº‘ç«¯ï¼ˆä»…ç»“æ„ï¼Œä¸åŒæ­¥æ•°æ®ï¼‰
3. ç”Ÿæˆè¯¦ç»†çš„åŒæ­¥æŠ¥å‘Š
æ³¨æ„ï¼šæ­¤å·¥å…·ä»…åŒæ­¥ç»“æ„ï¼Œä¸ä¼šå½±å“äº‘ç«¯æ•°æ®
"""

import os
import sys
import psycopg2
import logging
import subprocess
import datetime
from urllib.parse import urlparse

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger('ç»“æ„åŒæ­¥')

class PostgreSQLSchemaSync:
    def __init__(self):
        self.timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')
        self.backup_dir = os.path.join(os.path.dirname(__file__), '..', 'cloud_db_backups')
        os.makedirs(self.backup_dir, exist_ok=True)
        
        # æœ¬åœ°PostgreSQLæ•°æ®åº“URL (pma_local)
        self.local_db_url = "postgresql://nijie@localhost:5432/pma_local"
        
        # äº‘ç«¯PostgreSQLæ•°æ®åº“URL (pma_db_ovs)
        self.cloud_db_url = "postgresql://pma_db_ovs_user:oUKdxwqXDvCrgkg3fkZ33axXgDF21D51@dpg-d170laodl3ps739trgp0-a.singapore-postgres.render.com/pma_db_ovs"
    
    def parse_db_url(self, db_url):
        """è§£æPostgreSQLæ•°æ®åº“URL"""
        parsed = urlparse(db_url)
        return {
            'host': parsed.hostname,
            'port': parsed.port or 5432,
            'user': parsed.username,
            'password': parsed.password,
            'dbname': parsed.path.lstrip('/')
        }
    
    def test_connections(self):
        """æµ‹è¯•æœ¬åœ°å’Œäº‘ç«¯æ•°æ®åº“è¿æ¥"""
        logger.info("ğŸ”„ [1/6] æµ‹è¯•æ•°æ®åº“è¿æ¥...")
        
        # æµ‹è¯•æœ¬åœ°æ•°æ®åº“
        try:
            local_params = self.parse_db_url(self.local_db_url)
            conn = psycopg2.connect(**local_params)
            cursor = conn.cursor()
            cursor.execute("SELECT version();")
            version = cursor.fetchone()
            logger.info(f"âœ… æœ¬åœ°æ•°æ®åº“è¿æ¥æˆåŠŸ: {version[0][:50]}...")
            cursor.close()
            conn.close()
            local_ok = True
        except Exception as e:
            logger.error(f"âŒ æœ¬åœ°æ•°æ®åº“è¿æ¥å¤±è´¥: {str(e)}")
            local_ok = False
        
        # æµ‹è¯•äº‘ç«¯æ•°æ®åº“
        try:
            cloud_params = self.parse_db_url(self.cloud_db_url)
            conn = psycopg2.connect(**cloud_params)
            cursor = conn.cursor()
            cursor.execute("SELECT version();")
            version = cursor.fetchone()
            logger.info(f"âœ… äº‘ç«¯æ•°æ®åº“è¿æ¥æˆåŠŸ: {version[0][:50]}...")
            cursor.close()
            conn.close()
            cloud_ok = True
        except Exception as e:
            logger.error(f"âŒ äº‘ç«¯æ•°æ®åº“è¿æ¥å¤±è´¥: {str(e)}")
            cloud_ok = False
        
        return local_ok and cloud_ok
    
    def get_database_schema(self, db_url, db_name):
        """è·å–PostgreSQLæ•°æ®åº“ç»“æ„"""
        logger.info(f"ğŸ”„ [2/6] åˆ†æ{db_name}æ•°æ®åº“ç»“æ„...")
        
        try:
            db_params = self.parse_db_url(db_url)
            conn = psycopg2.connect(**db_params)
            cursor = conn.cursor()
            
            # è·å–æ‰€æœ‰è¡¨
            cursor.execute("""
                SELECT table_name 
                FROM information_schema.tables 
                WHERE table_schema = 'public' AND table_type = 'BASE TABLE'
                ORDER BY table_name
            """)
            tables = cursor.fetchall()
            
            schema_info = {}
            for (table_name,) in tables:
                # è·å–è¡¨ç»“æ„
                cursor.execute("""
                    SELECT column_name, data_type, is_nullable, column_default, 
                           character_maximum_length, numeric_precision, numeric_scale
                    FROM information_schema.columns 
                    WHERE table_schema = 'public' AND table_name = %s
                    ORDER BY ordinal_position
                """, (table_name,))
                columns = cursor.fetchall()
                
                # è·å–ä¸»é”®ä¿¡æ¯
                try:
                    cursor.execute("""
                        SELECT kcu.column_name
                        FROM information_schema.table_constraints tc
                        JOIN information_schema.key_column_usage kcu 
                            ON tc.constraint_name = kcu.constraint_name
                        WHERE tc.table_schema = 'public' 
                            AND tc.table_name = %s 
                            AND tc.constraint_type = 'PRIMARY KEY'
                    """, (table_name,))
                    primary_keys = [row[0] for row in cursor.fetchall()]
                except Exception as e:
                    primary_keys = []
                
                # è·å–å¤–é”®ä¿¡æ¯
                try:
                    cursor.execute("""
                        SELECT kcu.column_name, ccu.table_name, ccu.column_name
                        FROM information_schema.table_constraints tc
                        JOIN information_schema.key_column_usage kcu 
                            ON tc.constraint_name = kcu.constraint_name
                        JOIN information_schema.constraint_column_usage ccu 
                            ON tc.constraint_name = ccu.constraint_name
                        WHERE tc.table_schema = 'public' 
                            AND tc.table_name = %s 
                            AND tc.constraint_type = 'FOREIGN KEY'
                    """, (table_name,))
                    foreign_keys = cursor.fetchall()
                except Exception as e:
                    foreign_keys = []
                
                # è·å–ç´¢å¼•ä¿¡æ¯
                try:
                    cursor.execute("""
                        SELECT indexname, indexdef
                        FROM pg_indexes 
                        WHERE schemaname = 'public' AND tablename = %s
                        AND indexname NOT LIKE '%_pkey'
                    """, (table_name,))
                    indexes = cursor.fetchall()
                except Exception as e:
                    indexes = []
                
                schema_info[table_name] = {
                    'columns': columns,
                    'primary_keys': primary_keys,
                    'foreign_keys': foreign_keys,
                    'indexes': indexes
                }
            
            conn.close()
            logger.info(f"âœ… {db_name}æ•°æ®åº“: {len(tables)} ä¸ªè¡¨")
            return schema_info
            
        except Exception as e:
            logger.error(f"è·å–{db_name}æ•°æ®åº“ç»“æ„å¤±è´¥: {str(e)}")
            return None
    
    def compare_schemas(self, local_schema, cloud_schema):
        """å¯¹æ¯”æœ¬åœ°å’Œäº‘ç«¯æ•°æ®åº“ç»“æ„"""
        logger.info("ğŸ”„ [3/6] å¯¹æ¯”æ•°æ®åº“ç»“æ„å·®å¼‚...")
        
        differences = {
            'missing_tables': [],
            'extra_tables': [],
            'different_tables': [],
            'missing_columns': {},
            'extra_columns': {}
        }
        
        local_tables = set(local_schema.keys())
        cloud_tables = set(cloud_schema.keys())
        
        # äº‘ç«¯ç¼ºå°‘çš„è¡¨ï¼ˆéœ€è¦åˆ›å»ºï¼‰
        differences['missing_tables'] = list(local_tables - cloud_tables)
        
        # äº‘ç«¯å¤šä½™çš„è¡¨ï¼ˆæœ¬åœ°æ²¡æœ‰ï¼‰
        differences['extra_tables'] = list(cloud_tables - local_tables)
        
        # å…±åŒçš„è¡¨ï¼Œæ£€æŸ¥åˆ—å·®å¼‚
        common_tables = local_tables & cloud_tables
        for table in common_tables:
            local_cols = {col[0]: col for col in local_schema[table]['columns']}
            cloud_cols = {col[0]: col for col in cloud_schema[table]['columns']}
            
            missing_cols = set(local_cols.keys()) - set(cloud_cols.keys())
            extra_cols = set(cloud_cols.keys()) - set(local_cols.keys())
            
            if missing_cols or extra_cols:
                differences['different_tables'].append(table)
                differences['missing_columns'][table] = list(missing_cols)
                differences['extra_columns'][table] = list(extra_cols)
        
        logger.info(f"âœ… ç»“æ„å¯¹æ¯”å®Œæˆ:")
        logger.info(f"   - éœ€è¦åˆ›å»ºçš„è¡¨: {len(differences['missing_tables'])}")
        logger.info(f"   - äº‘ç«¯å¤šä½™çš„è¡¨: {len(differences['extra_tables'])}")
        logger.info(f"   - ç»“æ„ä¸åŒçš„è¡¨: {len(differences['different_tables'])}")
        
        return differences
    
    def backup_cloud_before_sync(self):
        """åŒæ­¥å‰å¤‡ä»½äº‘ç«¯æ•°æ®åº“"""
        logger.info("ğŸ”„ [4/6] åŒæ­¥å‰å¤‡ä»½äº‘ç«¯æ•°æ®åº“...")
        
        cloud_params = self.parse_db_url(self.cloud_db_url)
        backup_file = os.path.join(self.backup_dir, f'pma_db_ovs_backup_before_sync_{self.timestamp}.sql')
        
        try:
            cmd = [
                'pg_dump',
                '-h', cloud_params['host'],
                '-p', str(cloud_params['port']),
                '-U', cloud_params['user'],
                '-d', cloud_params['dbname'],
                '--verbose',
                '--clean',
                '--if-exists',
                '--no-owner',
                '--no-privileges',
                '-f', backup_file
            ]
            
            env = os.environ.copy()
            env['PGPASSWORD'] = cloud_params['password']
            
            result = subprocess.run(cmd, env=env, capture_output=True, text=True)
            
            if result.returncode == 0:
                logger.info(f"âœ… äº‘ç«¯æ•°æ®åº“å¤‡ä»½æˆåŠŸ: {backup_file}")
                return backup_file
            else:
                logger.error(f"âŒ å¤‡ä»½å¤±è´¥: {result.stderr}")
                return None
                
        except Exception as e:
            logger.error(f"å¤‡ä»½è¿‡ç¨‹å‡ºé”™: {str(e)}")
            return None
    
    def export_local_schema(self):
        """å¯¼å‡ºæœ¬åœ°æ•°æ®åº“ç»“æ„"""
        logger.info("ğŸ”„ [5/6] å¯¼å‡ºæœ¬åœ°æ•°æ®åº“ç»“æ„...")
        
        local_params = self.parse_db_url(self.local_db_url)
        schema_file = os.path.join(self.backup_dir, f'local_pma_schema_{self.timestamp}.sql')
        
        try:
            cmd = [
                'pg_dump',
                '-h', local_params['host'],
                '-p', str(local_params['port']),
                '-U', local_params['user'],
                '-d', local_params['dbname'],
                '--schema-only',
                '--no-owner',
                '--no-privileges',
                '-f', schema_file
            ]
            
            env = os.environ.copy()
            if local_params['password']:
                env['PGPASSWORD'] = local_params['password']
            
            result = subprocess.run(cmd, env=env, capture_output=True, text=True)
            
            if result.returncode == 0:
                logger.info(f"âœ… æœ¬åœ°ç»“æ„å¯¼å‡ºæˆåŠŸ: {schema_file}")
                return schema_file
            else:
                logger.error(f"âŒ ç»“æ„å¯¼å‡ºå¤±è´¥: {result.stderr}")
                return None
                
        except Exception as e:
            logger.error(f"å¯¼å‡ºè¿‡ç¨‹å‡ºé”™: {str(e)}")
            return None
    
    def sync_schema_to_cloud(self, differences, local_schema):
        """å°†æœ¬åœ°ç»“æ„åŒæ­¥åˆ°äº‘ç«¯ï¼ˆä»…åŒæ­¥ç¼ºå¤±çš„è¡¨å’Œå­—æ®µï¼‰"""
        logger.info("ğŸ”„ [6/6] åŒæ­¥ç»“æ„åˆ°äº‘ç«¯æ•°æ®åº“...")
        
        if not differences['missing_tables'] and not differences['missing_columns']:
            logger.info("âœ… äº‘ç«¯æ•°æ®åº“ç»“æ„å·²åŒ…å«æœ¬åœ°æ‰€æœ‰è¡¨å’Œå­—æ®µï¼Œæ— éœ€åŒæ­¥")
            return True
        
        try:
            cloud_params = self.parse_db_url(self.cloud_db_url)
            conn = psycopg2.connect(**cloud_params)
            conn.autocommit = False
            cursor = conn.cursor()
            
            sync_operations = []
            
            try:
                # 1. å¤„ç†ç¼ºå¤±çš„è¡¨
                for table_name in differences['missing_tables']:
                    logger.info(f"ğŸ”„ åˆ›å»ºç¼ºå¤±çš„è¡¨: {table_name}")
                    
                    # ä»æœ¬åœ°æ•°æ®åº“è·å–è¡¨çš„å®Œæ•´CREATEè¯­å¥
                    local_params = self.parse_db_url(self.local_db_url)
                    local_conn = psycopg2.connect(**local_params)
                    local_cursor = local_conn.cursor()
                    
                    # è·å–è¡¨ç»“æ„
                    table_info = local_schema[table_name]
                    
                    # æ„å»ºCREATE TABLEè¯­å¥
                    create_sql = self.build_create_table_sql(table_name, table_info)
                    
                    cursor.execute(create_sql)
                    sync_operations.append(f"åˆ›å»ºè¡¨: {table_name}")
                    logger.info(f"   âœ… è¡¨ {table_name} åˆ›å»ºæˆåŠŸ")
                    
                    local_conn.close()
                
                # 2. å¤„ç†ç¼ºå¤±çš„å­—æ®µ
                for table_name, missing_cols in differences['missing_columns'].items():
                    if missing_cols:
                        logger.info(f"ğŸ”„ ä¸ºè¡¨ {table_name} æ·»åŠ ç¼ºå¤±å­—æ®µ: {missing_cols}")
                        
                        table_info = local_schema[table_name]
                        local_columns = {col[0]: col for col in table_info['columns']}
                        
                        for col_name in missing_cols:
                            if col_name in local_columns:
                                col_info = local_columns[col_name]
                                alter_sql = self.build_alter_table_sql(table_name, col_name, col_info)
                                
                                try:
                                    cursor.execute(alter_sql)
                                    sync_operations.append(f"æ·»åŠ å­—æ®µ: {table_name}.{col_name}")
                                    logger.info(f"   âœ… å­—æ®µ {table_name}.{col_name} æ·»åŠ æˆåŠŸ")
                                except Exception as e:
                                    logger.warning(f"   âš ï¸ å­—æ®µ {table_name}.{col_name} æ·»åŠ å¤±è´¥: {str(e)}")
                
                # æäº¤æ‰€æœ‰æ›´æ”¹
                conn.commit()
                logger.info(f"âœ… ç»“æ„åŒæ­¥æˆåŠŸï¼Œæ‰§è¡Œäº† {len(sync_operations)} ä¸ªæ“ä½œ")
                
                for op in sync_operations:
                    logger.info(f"   - {op}")
                
                cursor.close()
                conn.close()
                return True
                
            except Exception as e:
                logger.error(f"ç»“æ„åŒæ­¥å¤±è´¥: {str(e)}")
                conn.rollback()
                cursor.close()
                conn.close()
                return False
                
        except Exception as e:
            logger.error(f"åŒæ­¥è¿‡ç¨‹å‡ºé”™: {str(e)}")
            return False
    
    def build_create_table_sql(self, table_name, table_info):
        """æ„å»ºCREATE TABLE SQLè¯­å¥"""
        columns = table_info['columns']
        primary_keys = table_info['primary_keys']
        
        column_defs = []
        for col in columns:
            col_name = col[0]
            data_type = col[1]
            is_nullable = col[2]
            default_val = col[3]
            
            # æ„å»ºåˆ—å®šä¹‰
            col_def = f"{col_name} {data_type}"
            
            if not is_nullable == 'YES':
                col_def += " NOT NULL"
            
            if default_val:
                col_def += f" DEFAULT {default_val}"
            
            column_defs.append(col_def)
        
        # æ·»åŠ ä¸»é”®çº¦æŸ
        if primary_keys:
            pk_constraint = f"PRIMARY KEY ({', '.join(primary_keys)})"
            column_defs.append(pk_constraint)
        
        create_sql = f"CREATE TABLE {table_name} (\n  {',\n  '.join(column_defs)}\n);"
        return create_sql
    
    def build_alter_table_sql(self, table_name, col_name, col_info):
        """æ„å»ºALTER TABLE ADD COLUMN SQLè¯­å¥"""
        data_type = col_info[1]
        is_nullable = col_info[2]
        default_val = col_info[3]
        
        alter_sql = f"ALTER TABLE {table_name} ADD COLUMN {col_name} {data_type}"
        
        if default_val:
            alter_sql += f" DEFAULT {default_val}"
        
        if is_nullable != 'YES':
            alter_sql += " NOT NULL"
        
        alter_sql += ";"
        return alter_sql
    
    def generate_sync_report(self, differences, backup_file, sync_success):
        """ç”ŸæˆåŒæ­¥æŠ¥å‘Š"""
        report_file = os.path.join(self.backup_dir, f'schema_sync_report_{self.timestamp}.md')
        
        report_content = f"""# PostgreSQLæ•°æ®åº“ç»“æ„åŒæ­¥æŠ¥å‘Š

## åŒæ­¥æ¦‚è¿°
- åŒæ­¥æ—¶é—´: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
- æºæ•°æ®åº“: æœ¬åœ°PostgreSQL (pma_local)
- ç›®æ ‡æ•°æ®åº“: äº‘ç«¯PostgreSQL (pma_db_ovs)
- åŒæ­¥çŠ¶æ€: {'æˆåŠŸ' if sync_success else 'å¤±è´¥'}

## ç»“æ„å·®å¼‚åˆ†æ

### éœ€è¦åˆ›å»ºçš„è¡¨ ({len(differences['missing_tables'])})
"""
        
        for table in differences['missing_tables']:
            report_content += f"- {table}\n"
        
        report_content += f"""
### äº‘ç«¯å¤šä½™çš„è¡¨ ({len(differences['extra_tables'])})
"""
        for table in differences['extra_tables']:
            report_content += f"- {table}\n"
        
        report_content += f"""
### éœ€è¦æ·»åŠ å­—æ®µçš„è¡¨ ({len(differences['missing_columns'])})
"""
        for table, cols in differences['missing_columns'].items():
            if cols:
                report_content += f"#### {table}\n"
                for col in cols:
                    report_content += f"- {col}\n"
                report_content += "\n"
        
        report_content += f"""
### äº‘ç«¯å¤šä½™å­—æ®µçš„è¡¨ ({len(differences['extra_columns'])})
"""
        for table, cols in differences['extra_columns'].items():
            if cols:
                report_content += f"#### {table}\n"
                for col in cols:
                    report_content += f"- {col}\n"
                report_content += "\n"
        
        report_content += f"""
## æ‰§è¡Œæ­¥éª¤
1. âœ… æµ‹è¯•æ•°æ®åº“è¿æ¥
2. âœ… åˆ†ææœ¬åœ°PostgreSQLæ•°æ®åº“ç»“æ„  
3. âœ… åˆ†æäº‘ç«¯PostgreSQLæ•°æ®åº“ç»“æ„
4. âœ… å¯¹æ¯”æ•°æ®åº“ç»“æ„å·®å¼‚
5. {'âœ…' if backup_file else 'âŒ'} åŒæ­¥å‰å¤‡ä»½äº‘ç«¯æ•°æ®åº“
6. âœ… å¯¼å‡ºæœ¬åœ°æ•°æ®åº“ç»“æ„
7. {'âœ…' if sync_success else 'âŒ'} åŒæ­¥ç»“æ„åˆ°äº‘ç«¯

## æ–‡ä»¶ä½ç½®
- å¤‡ä»½ç›®å½•: {self.backup_dir}
- äº‘ç«¯å¤‡ä»½: {backup_file or 'æ— '}
- æœ¬åœ°æ•°æ®åº“: {self.local_db_url}
- äº‘ç«¯æ•°æ®åº“: pma_db_ovs

## å®‰å…¨ç¡®è®¤
- âœ… ä»…åŒæ­¥æ•°æ®åº“ç»“æ„ï¼ŒæœªåŒæ­¥æ•°æ®
- âœ… åŒæ­¥å‰å·²å¤‡ä»½äº‘ç«¯æ•°æ®åº“
- âœ… åªæ·»åŠ ç¼ºå¤±çš„è¡¨å’Œå­—æ®µï¼Œä¸åˆ é™¤ç°æœ‰å†…å®¹
- âœ… äº‘ç«¯æ•°æ®å®Œå…¨å®‰å…¨
- âœ… æ‰€æœ‰æ“ä½œå¯å›æ»š
"""
        
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report_content)
        
        logger.info(f"ğŸ“‹ åŒæ­¥æŠ¥å‘Šå·²ç”Ÿæˆ: {report_file}")
        return report_file
    
    def run(self):
        """æ‰§è¡Œå®Œæ•´çš„ç»“æ„åŒæ­¥æµç¨‹"""
        logger.info("ğŸš€ å¼€å§‹PostgreSQLæ•°æ®åº“ç»“æ„åŒæ­¥æµç¨‹...")
        
        # 1. æµ‹è¯•è¿æ¥
        if not self.test_connections():
            logger.error("âŒ æ•°æ®åº“è¿æ¥æµ‹è¯•å¤±è´¥")
            return False
        
        # 2. è·å–æœ¬åœ°ç»“æ„
        local_schema = self.get_database_schema(self.local_db_url, "æœ¬åœ°")
        if not local_schema:
            logger.error("âŒ è·å–æœ¬åœ°æ•°æ®åº“ç»“æ„å¤±è´¥")
            return False
        
        # 3. è·å–äº‘ç«¯ç»“æ„
        cloud_schema = self.get_database_schema(self.cloud_db_url, "äº‘ç«¯")
        if not cloud_schema:
            logger.error("âŒ è·å–äº‘ç«¯æ•°æ®åº“ç»“æ„å¤±è´¥")
            return False
        
        # 4. å¯¹æ¯”ç»“æ„
        differences = self.compare_schemas(local_schema, cloud_schema)
        
        # 5. å¤‡ä»½äº‘ç«¯æ•°æ®åº“
        backup_file = self.backup_cloud_before_sync()
        
        # 6. å¯¼å‡ºæœ¬åœ°ç»“æ„
        schema_file = self.export_local_schema()
        
        # 7. åŒæ­¥ç»“æ„åˆ°äº‘ç«¯
        sync_success = self.sync_schema_to_cloud(differences, local_schema)
        
        # 8. ç”ŸæˆæŠ¥å‘Š
        report_file = self.generate_sync_report(differences, backup_file, sync_success)
        
        if sync_success:
            logger.info("ğŸ‰ PostgreSQLæ•°æ®åº“ç»“æ„åŒæ­¥å®Œæˆ!")
            logger.info(f"ğŸ“‹ è¯¦ç»†æŠ¥å‘Š: {report_file}")
        else:
            logger.error("âŒ ç»“æ„åŒæ­¥å¤±è´¥ï¼Œè¯·æŸ¥çœ‹æŠ¥å‘Š")
        
        return sync_success

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description='æœ¬åœ°PostgreSQLæ•°æ®åº“ç»“æ„åŒæ­¥åˆ°äº‘ç«¯pma_db_ovs')
    parser.add_argument('--dry-run', action='store_true', help='ä»…å¯¹æ¯”ç»“æ„ï¼Œä¸æ‰§è¡ŒåŒæ­¥')
    args = parser.parse_args()
    
    sync_tool = PostgreSQLSchemaSync()
    
    if args.dry_run:
        logger.info("æ‰§è¡Œè¯•è¿è¡Œæ¨¡å¼ï¼Œä»…å¯¹æ¯”ç»“æ„...")
        # æµ‹è¯•è¿æ¥
        if not sync_tool.test_connections():
            sys.exit(1)
        
        # è·å–ç»“æ„å¹¶å¯¹æ¯”
        local_schema = sync_tool.get_database_schema(sync_tool.local_db_url, "æœ¬åœ°")
        cloud_schema = sync_tool.get_database_schema(sync_tool.cloud_db_url, "äº‘ç«¯")
        
        if local_schema and cloud_schema:
            differences = sync_tool.compare_schemas(local_schema, cloud_schema)
            sync_tool.generate_sync_report(differences, None, False)
        else:
            logger.error("âŒ æ— æ³•è·å–æ•°æ®åº“ç»“æ„")
    else:
        success = sync_tool.run()
        if not success:
            sys.exit(1)