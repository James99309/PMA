#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ™ºèƒ½æ•°æ®åº“ç»“æ„åŒæ­¥è„šæœ¬
åŠŸèƒ½ï¼š
1. æ¯”è¾ƒæœ¬åœ°å’Œäº‘ç«¯æ•°æ®åº“ç»“æ„å·®å¼‚
2. ç”Ÿæˆå¢é‡æ›´æ–°SQL
3. å®‰å…¨åœ°åŒæ­¥ç»“æ„åˆ°äº‘ç«¯
"""

import os
import sys
import psycopg2
import logging
import subprocess
import datetime
from urllib.parse import urlparse
import re

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger('æ™ºèƒ½ç»“æ„åŒæ­¥')

class SmartSchemaSync:
    def __init__(self):
        self.timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')
        self.backup_dir = os.path.join(os.path.dirname(__file__), '..', 'cloud_db_backups')
        os.makedirs(self.backup_dir, exist_ok=True)
        
        # æ•°æ®åº“è¿æ¥ä¿¡æ¯
        self.local_db_url = os.environ.get('DATABASE_URL')
        self.cloud_db_url = os.environ.get('RENDER_DB_URL')
        
        if not self.local_db_url:
            logger.error("æœªæ‰¾åˆ°æœ¬åœ°æ•°æ®åº“URLï¼Œè¯·è®¾ç½®DATABASE_URLç¯å¢ƒå˜é‡")
            sys.exit(1)
            
        if not self.cloud_db_url:
            logger.error("æœªæ‰¾åˆ°äº‘ç«¯æ•°æ®åº“URLï¼Œè¯·è®¾ç½®RENDER_DB_URLç¯å¢ƒå˜é‡")
            sys.exit(1)
    
    def parse_db_url(self, db_url):
        """è§£ææ•°æ®åº“URL"""
        parsed = urlparse(db_url)
        return {
            'host': parsed.hostname,
            'port': parsed.port or 5432,
            'user': parsed.username,
            'password': parsed.password,
            'dbname': parsed.path.lstrip('/')
        }
    
    def get_db_schema_info(self, db_params, db_name):
        """è·å–æ•°æ®åº“ç»“æ„ä¿¡æ¯"""
        logger.info(f"è·å–{db_name}æ•°æ®åº“ç»“æ„ä¿¡æ¯...")
        
        try:
            conn = psycopg2.connect(**db_params)
            cursor = conn.cursor()
            
            schema_info = {
                'tables': {},
                'columns': {},
                'indexes': {},
                'constraints': {},
                'types': set(),
                'functions': set()
            }
            
            # è·å–è¡¨ä¿¡æ¯
            cursor.execute("""
                SELECT table_name 
                FROM information_schema.tables 
                WHERE table_schema = 'public' 
                ORDER BY table_name;
            """)
            tables = cursor.fetchall()
            for (table_name,) in tables:
                schema_info['tables'][table_name] = True
            
            # è·å–åˆ—ä¿¡æ¯
            cursor.execute("""
                SELECT table_name, column_name, data_type, is_nullable, column_default
                FROM information_schema.columns 
                WHERE table_schema = 'public'
                ORDER BY table_name, ordinal_position;
            """)
            columns = cursor.fetchall()
            for table_name, column_name, data_type, is_nullable, column_default in columns:
                if table_name not in schema_info['columns']:
                    schema_info['columns'][table_name] = {}
                schema_info['columns'][table_name][column_name] = {
                    'data_type': data_type,
                    'is_nullable': is_nullable,
                    'column_default': column_default
                }
            
            # è·å–è‡ªå®šä¹‰ç±»å‹
            cursor.execute("""
                SELECT typname FROM pg_type 
                WHERE typtype = 'e' AND typnamespace = (
                    SELECT oid FROM pg_namespace WHERE nspname = 'public'
                );
            """)
            types = cursor.fetchall()
            for (type_name,) in types:
                schema_info['types'].add(type_name)
            
            # è·å–ç´¢å¼•ä¿¡æ¯
            cursor.execute("""
                SELECT indexname, tablename, indexdef
                FROM pg_indexes 
                WHERE schemaname = 'public'
                ORDER BY tablename, indexname;
            """)
            indexes = cursor.fetchall()
            for index_name, table_name, index_def in indexes:
                if table_name not in schema_info['indexes']:
                    schema_info['indexes'][table_name] = {}
                schema_info['indexes'][table_name][index_name] = index_def
            
            cursor.close()
            conn.close()
            
            logger.info(f"{db_name}æ•°æ®åº“ç»“æ„ä¿¡æ¯è·å–å®Œæˆ")
            logger.info(f"  - è¡¨æ•°é‡: {len(schema_info['tables'])}")
            logger.info(f"  - è‡ªå®šä¹‰ç±»å‹: {len(schema_info['types'])}")
            
            return schema_info
            
        except Exception as e:
            logger.error(f"è·å–{db_name}æ•°æ®åº“ç»“æ„å¤±è´¥: {str(e)}")
            return None
    
    def compare_schemas(self, local_schema, cloud_schema):
        """æ¯”è¾ƒæœ¬åœ°å’Œäº‘ç«¯æ•°æ®åº“ç»“æ„"""
        logger.info("æ¯”è¾ƒæœ¬åœ°å’Œäº‘ç«¯æ•°æ®åº“ç»“æ„...")
        
        differences = {
            'new_tables': [],
            'new_columns': [],
            'missing_types': [],
            'new_indexes': []
        }
        
        # æ¯”è¾ƒè¡¨
        for table_name in local_schema['tables']:
            if table_name not in cloud_schema['tables']:
                differences['new_tables'].append(table_name)
        
        # æ¯”è¾ƒåˆ—
        for table_name, columns in local_schema['columns'].items():
            if table_name in cloud_schema['columns']:
                cloud_columns = cloud_schema['columns'][table_name]
                for column_name, column_info in columns.items():
                    if column_name not in cloud_columns:
                        differences['new_columns'].append({
                            'table': table_name,
                            'column': column_name,
                            'info': column_info
                        })
        
        # æ¯”è¾ƒè‡ªå®šä¹‰ç±»å‹
        for type_name in local_schema['types']:
            if type_name not in cloud_schema['types']:
                differences['missing_types'].append(type_name)
        
        # æ¯”è¾ƒç´¢å¼•
        for table_name, indexes in local_schema['indexes'].items():
            if table_name in cloud_schema['indexes']:
                cloud_indexes = cloud_schema['indexes'][table_name]
                for index_name, index_def in indexes.items():
                    if index_name not in cloud_indexes:
                        differences['new_indexes'].append({
                            'table': table_name,
                            'index': index_name,
                            'definition': index_def
                        })
        
        logger.info("ç»“æ„æ¯”è¾ƒå®Œæˆ:")
        logger.info(f"  - æ–°å¢è¡¨: {len(differences['new_tables'])}")
        logger.info(f"  - æ–°å¢åˆ—: {len(differences['new_columns'])}")
        logger.info(f"  - ç¼ºå¤±ç±»å‹: {len(differences['missing_types'])}")
        logger.info(f"  - æ–°å¢ç´¢å¼•: {len(differences['new_indexes'])}")
        
        return differences
    
    def generate_migration_sql(self, differences):
        """ç”Ÿæˆè¿ç§»SQL"""
        logger.info("ç”Ÿæˆè¿ç§»SQL...")
        
        migration_sql = []
        
        # æ·»åŠ ç¼ºå¤±çš„è‡ªå®šä¹‰ç±»å‹
        if differences['missing_types']:
            migration_sql.append("-- æ·»åŠ ç¼ºå¤±çš„è‡ªå®šä¹‰ç±»å‹")
            for type_name in differences['missing_types']:
                # è¿™é‡Œéœ€è¦ä»æœ¬åœ°æ•°æ®åº“è·å–ç±»å‹å®šä¹‰
                type_def = self.get_type_definition(type_name)
                if type_def:
                    migration_sql.append(f"DO $$ BEGIN")
                    migration_sql.append(f"    IF NOT EXISTS (SELECT 1 FROM pg_type WHERE typname = '{type_name}') THEN")
                    migration_sql.append(f"        {type_def};")
                    migration_sql.append(f"    END IF;")
                    migration_sql.append(f"END $$;")
                    migration_sql.append("")
        
        # æ·»åŠ æ–°å¢çš„åˆ—
        if differences['new_columns']:
            migration_sql.append("-- æ·»åŠ æ–°å¢çš„åˆ—")
            for col_info in differences['new_columns']:
                table = col_info['table']
                column = col_info['column']
                info = col_info['info']
                
                alter_sql = f"ALTER TABLE {table} ADD COLUMN IF NOT EXISTS {column} {info['data_type']}"
                
                if info['is_nullable'] == 'NO':
                    alter_sql += " NOT NULL"
                
                if info['column_default']:
                    alter_sql += f" DEFAULT {info['column_default']}"
                
                migration_sql.append(alter_sql + ";")
            migration_sql.append("")
        
        # æ·»åŠ æ–°å¢çš„ç´¢å¼•
        if differences['new_indexes']:
            migration_sql.append("-- æ·»åŠ æ–°å¢çš„ç´¢å¼•")
            for idx_info in differences['new_indexes']:
                # ä¿®æ”¹ç´¢å¼•å®šä¹‰ï¼Œæ·»åŠ IF NOT EXISTS
                index_def = idx_info['definition']
                if 'CREATE INDEX' in index_def:
                    index_def = index_def.replace('CREATE INDEX', 'CREATE INDEX IF NOT EXISTS')
                elif 'CREATE UNIQUE INDEX' in index_def:
                    index_def = index_def.replace('CREATE UNIQUE INDEX', 'CREATE UNIQUE INDEX IF NOT EXISTS')
                
                migration_sql.append(index_def + ";")
            migration_sql.append("")
        
        return "\n".join(migration_sql)
    
    def get_type_definition(self, type_name):
        """è·å–è‡ªå®šä¹‰ç±»å‹å®šä¹‰"""
        try:
            local_params = self.parse_db_url(self.local_db_url)
            conn = psycopg2.connect(**local_params)
            cursor = conn.cursor()
            
            cursor.execute("""
                SELECT string_agg(enumlabel, ''', ''' ORDER BY enumsortorder) as enum_values
                FROM pg_enum 
                WHERE enumtypid = (
                    SELECT oid FROM pg_type WHERE typname = %s
                );
            """, (type_name,))
            
            result = cursor.fetchone()
            if result and result[0]:
                enum_values = result[0]
                type_def = f"CREATE TYPE {type_name} AS ENUM ('{enum_values}')"
                cursor.close()
                conn.close()
                return type_def
            
            cursor.close()
            conn.close()
            return None
            
        except Exception as e:
            logger.error(f"è·å–ç±»å‹å®šä¹‰å¤±è´¥: {str(e)}")
            return None
    
    def apply_migration(self, migration_sql):
        """åº”ç”¨è¿ç§»SQLåˆ°äº‘ç«¯æ•°æ®åº“"""
        logger.info("åº”ç”¨è¿ç§»SQLåˆ°äº‘ç«¯æ•°æ®åº“...")
        
        if not migration_sql.strip():
            logger.info("æ²¡æœ‰éœ€è¦è¿ç§»çš„ç»“æ„å˜æ›´")
            return True
        
        try:
            cloud_params = self.parse_db_url(self.cloud_db_url)
            conn = psycopg2.connect(**cloud_params)
            conn.autocommit = False
            cursor = conn.cursor()
            
            try:
                # æ‰§è¡Œè¿ç§»SQL
                cursor.execute(migration_sql)
                conn.commit()
                logger.info("è¿ç§»SQLæ‰§è¡ŒæˆåŠŸ")
                
                cursor.close()
                conn.close()
                return True
                
            except Exception as e:
                logger.error(f"è¿ç§»æ‰§è¡Œå¤±è´¥: {str(e)}")
                conn.rollback()
                cursor.close()
                conn.close()
                return False
                
        except Exception as e:
            logger.error(f"è¿æ¥äº‘ç«¯æ•°æ®åº“å¤±è´¥: {str(e)}")
            return False
    
    def save_migration_files(self, migration_sql, differences):
        """ä¿å­˜è¿ç§»æ–‡ä»¶"""
        migration_file = os.path.join(self.backup_dir, f'migration_{self.timestamp}.sql')
        report_file = os.path.join(self.backup_dir, f'migration_report_{self.timestamp}.md')
        
        # ä¿å­˜è¿ç§»SQL
        with open(migration_file, 'w', encoding='utf-8') as f:
            f.write(migration_sql)
        
        # ç”Ÿæˆè¿ç§»æŠ¥å‘Š
        report_content = f"""# æ•°æ®åº“ç»“æ„è¿ç§»æŠ¥å‘Š

## è¿ç§»æ¦‚è¿°
- è¿ç§»æ—¶é—´: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
- è¿ç§»æ–‡ä»¶: {os.path.basename(migration_file)}

## ç»“æ„å˜æ›´ç»Ÿè®¡
- æ–°å¢è¡¨: {len(differences['new_tables'])}
- æ–°å¢åˆ—: {len(differences['new_columns'])}
- ç¼ºå¤±ç±»å‹: {len(differences['missing_types'])}
- æ–°å¢ç´¢å¼•: {len(differences['new_indexes'])}

## è¯¦ç»†å˜æ›´
"""
        
        if differences['new_tables']:
            report_content += "\n### æ–°å¢è¡¨\n"
            for table in differences['new_tables']:
                report_content += f"- {table}\n"
        
        if differences['new_columns']:
            report_content += "\n### æ–°å¢åˆ—\n"
            for col_info in differences['new_columns']:
                report_content += f"- {col_info['table']}.{col_info['column']} ({col_info['info']['data_type']})\n"
        
        if differences['missing_types']:
            report_content += "\n### ç¼ºå¤±ç±»å‹\n"
            for type_name in differences['missing_types']:
                report_content += f"- {type_name}\n"
        
        if differences['new_indexes']:
            report_content += "\n### æ–°å¢ç´¢å¼•\n"
            for idx_info in differences['new_indexes']:
                report_content += f"- {idx_info['table']}.{idx_info['index']}\n"
        
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report_content)
        
        logger.info(f"è¿ç§»æ–‡ä»¶å·²ä¿å­˜: {migration_file}")
        logger.info(f"è¿ç§»æŠ¥å‘Šå·²ä¿å­˜: {report_file}")
        
        return migration_file, report_file
    
    def run(self):
        """æ‰§è¡Œæ™ºèƒ½ç»“æ„åŒæ­¥"""
        logger.info("å¼€å§‹æ™ºèƒ½æ•°æ®åº“ç»“æ„åŒæ­¥...")
        
        # 1. è·å–æœ¬åœ°æ•°æ®åº“ç»“æ„
        local_params = self.parse_db_url(self.local_db_url)
        local_schema = self.get_db_schema_info(local_params, "æœ¬åœ°")
        if not local_schema:
            return False
        
        # 2. è·å–äº‘ç«¯æ•°æ®åº“ç»“æ„
        cloud_params = self.parse_db_url(self.cloud_db_url)
        cloud_schema = self.get_db_schema_info(cloud_params, "äº‘ç«¯")
        if not cloud_schema:
            return False
        
        # 3. æ¯”è¾ƒç»“æ„å·®å¼‚
        differences = self.compare_schemas(local_schema, cloud_schema)
        
        # 4. ç”Ÿæˆè¿ç§»SQL
        migration_sql = self.generate_migration_sql(differences)
        
        # 5. ä¿å­˜è¿ç§»æ–‡ä»¶
        migration_file, report_file = self.save_migration_files(migration_sql, differences)
        
        # 6. åº”ç”¨è¿ç§»
        if migration_sql.strip():
            logger.info("å‘ç°ç»“æ„å·®å¼‚ï¼Œå‡†å¤‡åº”ç”¨è¿ç§»...")
            success = self.apply_migration(migration_sql)
            
            if success:
                logger.info("ğŸ‰ æ•°æ®åº“ç»“æ„åŒæ­¥å®Œæˆ!")
                logger.info(f"ğŸ“‹ è¿ç§»æŠ¥å‘Š: {report_file}")
            else:
                logger.error("âŒ è¿ç§»åº”ç”¨å¤±è´¥")
            
            return success
        else:
            logger.info("âœ… æ•°æ®åº“ç»“æ„å·²åŒæ­¥ï¼Œæ— éœ€è¿ç§»")
            return True

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description='æ™ºèƒ½æ•°æ®åº“ç»“æ„åŒæ­¥')
    parser.add_argument('--dry-run', action='store_true', help='ä»…æ¯”è¾ƒç»“æ„ï¼Œä¸æ‰§è¡Œè¿ç§»')
    args = parser.parse_args()
    
    sync_tool = SmartSchemaSync()
    
    if args.dry_run:
        logger.info("æ‰§è¡Œè¯•è¿è¡Œæ¨¡å¼ï¼Œä»…æ¯”è¾ƒç»“æ„...")
        # è¿™é‡Œå¯ä»¥æ·»åŠ è¯•è¿è¡Œé€»è¾‘
    else:
        sync_tool.run() 