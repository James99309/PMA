#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
äº‘ç«¯æ•°æ®åº“å¤‡ä»½å’Œæœ¬åœ°ç»“æ„åŒæ­¥è„šæœ¬
åŠŸèƒ½ï¼š
1. å¤‡ä»½äº‘ç«¯ pma_db_ovs æ•°æ®åº“
2. è·å–æœ¬åœ°æ•°æ®åº“ç»“æ„
3. å°†æœ¬åœ°ç»“æ„åŒæ­¥åˆ°äº‘ç«¯æ•°æ®åº“
"""

import os
import sys
import psycopg2
import logging
import subprocess
import datetime
from urllib.parse import urlparse
import json

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger('äº‘ç«¯æ•°æ®åº“åŒæ­¥')

class CloudDatabaseSync:
    def __init__(self):
        self.timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')
        self.backup_dir = os.path.join(os.path.dirname(__file__), '..', 'cloud_db_backups')
        os.makedirs(self.backup_dir, exist_ok=True)
        
        # æ•°æ®åº“è¿æ¥ä¿¡æ¯
        self.local_db_url = os.environ.get('DATABASE_URL')
        self.cloud_db_url = os.environ.get('RENDER_DB_URL')
        
        if not self.local_db_url:
            logger.error("æœªæ‰¾åˆ°æœ¬åœ°æ•°æ®åº“URLï¼Œè¯·è®¾ç½®DATABASE_URLç¯å¢ƒå˜é‡")
            sys.exit(1)
            
        if not self.cloud_db_url:
            logger.error("æœªæ‰¾åˆ°äº‘ç«¯æ•°æ®åº“URLï¼Œè¯·è®¾ç½®RENDER_DB_URLç¯å¢ƒå˜é‡")
            sys.exit(1)
    
    def parse_db_url(self, db_url):
        """è§£ææ•°æ®åº“URL"""
        parsed = urlparse(db_url)
        return {
            'host': parsed.hostname,
            'port': parsed.port or 5432,
            'user': parsed.username,
            'password': parsed.password,
            'dbname': parsed.path.lstrip('/')
        }
    
    def test_connection(self, db_url, name):
        """æµ‹è¯•æ•°æ®åº“è¿æ¥"""
        logger.info(f"æµ‹è¯•{name}æ•°æ®åº“è¿æ¥...")
        try:
            db_params = self.parse_db_url(db_url)
            conn = psycopg2.connect(**db_params)
            cursor = conn.cursor()
            cursor.execute("SELECT version();")
            version = cursor.fetchone()
            logger.info(f"{name}æ•°æ®åº“è¿æ¥æˆåŠŸ: {version[0][:50]}...")
            cursor.close()
            conn.close()
            return True
        except Exception as e:
            logger.error(f"{name}æ•°æ®åº“è¿æ¥å¤±è´¥: {str(e)}")
            return False
    
    def backup_cloud_database(self):
        """å¤‡ä»½äº‘ç«¯æ•°æ®åº“"""
        logger.info("å¼€å§‹å¤‡ä»½äº‘ç«¯æ•°æ®åº“...")
        
        cloud_params = self.parse_db_url(self.cloud_db_url)
        backup_file = os.path.join(self.backup_dir, f'pma_db_ovs_backup_{self.timestamp}.sql')
        info_file = os.path.join(self.backup_dir, f'pma_db_ovs_backup_info_{self.timestamp}.md')
        
        try:
            # ä½¿ç”¨pg_dumpå¤‡ä»½æ•°æ®åº“
            cmd = [
                'pg_dump',
                '-h', cloud_params['host'],
                '-p', str(cloud_params['port']),
                '-U', cloud_params['user'],
                '-d', cloud_params['dbname'],
                '--verbose',
                '--clean',
                '--if-exists',
                '--no-owner',
                '--no-privileges',
                '-f', backup_file
            ]
            
            # è®¾ç½®å¯†ç ç¯å¢ƒå˜é‡
            env = os.environ.copy()
            env['PGPASSWORD'] = cloud_params['password']
            
            logger.info(f"æ‰§è¡Œå¤‡ä»½å‘½ä»¤: {' '.join(cmd[:-2])} [å¯†ç å·²éšè—]")
            
            result = subprocess.run(cmd, env=env, capture_output=True, text=True)
            
            if result.returncode == 0:
                logger.info(f"äº‘ç«¯æ•°æ®åº“å¤‡ä»½æˆåŠŸ: {backup_file}")
                
                # ç”Ÿæˆå¤‡ä»½ä¿¡æ¯æ–‡ä»¶
                self.generate_backup_info(cloud_params, backup_file, info_file)
                
                return backup_file
            else:
                logger.error(f"å¤‡ä»½å¤±è´¥: {result.stderr}")
                return None
                
        except Exception as e:
            logger.error(f"å¤‡ä»½è¿‡ç¨‹å‡ºé”™: {str(e)}")
            return None
    
    def generate_backup_info(self, db_params, backup_file, info_file):
        """ç”Ÿæˆå¤‡ä»½ä¿¡æ¯æ–‡ä»¶"""
        try:
            conn = psycopg2.connect(**db_params)
            cursor = conn.cursor()
            
            # è·å–æ•°æ®åº“ä¿¡æ¯
            cursor.execute("SELECT version();")
            db_version = cursor.fetchone()[0]
            
            cursor.execute("SELECT pg_size_pretty(pg_database_size(current_database()));")
            db_size = cursor.fetchone()[0]
            
            # ä¿®å¤è¡¨ç»Ÿè®¡æŸ¥è¯¢
            cursor.execute("""
                SELECT schemaname, relname, n_tup_ins, n_tup_upd, n_tup_del 
                FROM pg_stat_user_tables 
                ORDER BY schemaname, relname;
            """)
            table_stats = cursor.fetchall()
            
            cursor.execute("""
                SELECT table_name, column_name, data_type, is_nullable
                FROM information_schema.columns 
                WHERE table_schema = 'public'
                ORDER BY table_name, ordinal_position;
            """)
            columns_info = cursor.fetchall()
            
            # ç”Ÿæˆä¿¡æ¯æ–‡æ¡£
            info_content = f"""# äº‘ç«¯æ•°æ®åº“å¤‡ä»½ä¿¡æ¯

## å¤‡ä»½åŸºæœ¬ä¿¡æ¯
- å¤‡ä»½æ—¶é—´: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
- å¤‡ä»½æ–‡ä»¶: {os.path.basename(backup_file)}
- æ•°æ®åº“ä¸»æœº: {db_params['host']}
- æ•°æ®åº“åç§°: {db_params['dbname']}
- æ•°æ®åº“ç‰ˆæœ¬: {db_version}
- æ•°æ®åº“å¤§å°: {db_size}

## è¡¨ç»Ÿè®¡ä¿¡æ¯
| è¡¨å | æ’å…¥è¡Œæ•° | æ›´æ–°è¡Œæ•° | åˆ é™¤è¡Œæ•° |
|------|----------|----------|----------|
"""
            
            for schema, table, inserts, updates, deletes in table_stats:
                info_content += f"| {table} | {inserts or 0} | {updates or 0} | {deletes or 0} |\n"
            
            info_content += "\n## è¡¨ç»“æ„ä¿¡æ¯\n"
            current_table = None
            for table, column, data_type, nullable in columns_info:
                if table != current_table:
                    info_content += f"\n### {table}\n| å­—æ®µå | æ•°æ®ç±»å‹ | å¯ç©º |\n|--------|----------|------|\n"
                    current_table = table
                info_content += f"| {column} | {data_type} | {nullable} |\n"
            
            with open(info_file, 'w', encoding='utf-8') as f:
                f.write(info_content)
            
            logger.info(f"å¤‡ä»½ä¿¡æ¯æ–‡ä»¶å·²ç”Ÿæˆ: {info_file}")
            
            cursor.close()
            conn.close()
            
        except Exception as e:
            logger.error(f"ç”Ÿæˆå¤‡ä»½ä¿¡æ¯å¤±è´¥: {str(e)}")
    
    def get_local_schema(self):
        """è·å–æœ¬åœ°æ•°æ®åº“ç»“æ„"""
        logger.info("è·å–æœ¬åœ°æ•°æ®åº“ç»“æ„...")
        
        local_params = self.parse_db_url(self.local_db_url)
        schema_file = os.path.join(self.backup_dir, f'local_schema_{self.timestamp}.sql')
        
        try:
            cmd = [
                'pg_dump',
                '-h', local_params['host'],
                '-p', str(local_params['port']),
                '-U', local_params['user'],
                '-d', local_params['dbname'],
                '--schema-only',
                '--no-owner',
                '--no-privileges',
                '-f', schema_file
            ]
            
            env = os.environ.copy()
            if local_params['password']:
                env['PGPASSWORD'] = local_params['password']
            
            result = subprocess.run(cmd, env=env, capture_output=True, text=True)
            
            if result.returncode == 0:
                logger.info(f"æœ¬åœ°æ•°æ®åº“ç»“æ„å¯¼å‡ºæˆåŠŸ: {schema_file}")
                return schema_file
            else:
                logger.error(f"æœ¬åœ°ç»“æ„å¯¼å‡ºå¤±è´¥: {result.stderr}")
                return None
                
        except Exception as e:
            logger.error(f"è·å–æœ¬åœ°ç»“æ„å‡ºé”™: {str(e)}")
            return None
    
    def sync_schema_to_cloud(self, schema_file):
        """å°†æœ¬åœ°ç»“æ„åŒæ­¥åˆ°äº‘ç«¯æ•°æ®åº“"""
        logger.info("å¼€å§‹åŒæ­¥æœ¬åœ°ç»“æ„åˆ°äº‘ç«¯æ•°æ®åº“...")
        
        cloud_params = self.parse_db_url(self.cloud_db_url)
        
        try:
            # è¯»å–ç»“æ„æ–‡ä»¶
            with open(schema_file, 'r', encoding='utf-8') as f:
                schema_sql = f.read()
            
            # è¿æ¥äº‘ç«¯æ•°æ®åº“
            conn = psycopg2.connect(**cloud_params)
            conn.autocommit = False
            cursor = conn.cursor()
            
            try:
                # æ‰§è¡Œç»“æ„åŒæ­¥
                logger.info("æ‰§è¡Œç»“æ„åŒæ­¥SQL...")
                cursor.execute(schema_sql)
                conn.commit()
                logger.info("ç»“æ„åŒæ­¥æˆåŠŸ")
                
                # éªŒè¯åŒæ­¥ç»“æœ
                cursor.execute("""
                    SELECT table_name FROM information_schema.tables 
                    WHERE table_schema = 'public' 
                    ORDER BY table_name;
                """)
                tables = cursor.fetchall()
                logger.info(f"äº‘ç«¯æ•°æ®åº“ç°æœ‰è¡¨: {', '.join([t[0] for t in tables])}")
                
                return True
                
            except Exception as e:
                logger.error(f"ç»“æ„åŒæ­¥å¤±è´¥: {str(e)}")
                conn.rollback()
                return False
            finally:
                cursor.close()
                conn.close()
                
        except Exception as e:
            logger.error(f"åŒæ­¥è¿‡ç¨‹å‡ºé”™: {str(e)}")
            return False
    
    def generate_sync_report(self, backup_file, schema_file, sync_success):
        """ç”ŸæˆåŒæ­¥æŠ¥å‘Š"""
        report_file = os.path.join(self.backup_dir, f'sync_report_{self.timestamp}.md')
        
        report_content = f"""# äº‘ç«¯æ•°æ®åº“åŒæ­¥æŠ¥å‘Š

## åŒæ­¥æ¦‚è¿°
- åŒæ­¥æ—¶é—´: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
- å¤‡ä»½æ–‡ä»¶: {os.path.basename(backup_file) if backup_file else 'å¤‡ä»½å¤±è´¥'}
- ç»“æ„æ–‡ä»¶: {os.path.basename(schema_file) if schema_file else 'ç»“æ„å¯¼å‡ºå¤±è´¥'}
- åŒæ­¥çŠ¶æ€: {'æˆåŠŸ' if sync_success else 'å¤±è´¥'}

## æ‰§è¡Œæ­¥éª¤
1. âœ… æµ‹è¯•æ•°æ®åº“è¿æ¥
2. {'âœ…' if backup_file else 'âŒ'} å¤‡ä»½äº‘ç«¯æ•°æ®åº“
3. {'âœ…' if schema_file else 'âŒ'} å¯¼å‡ºæœ¬åœ°æ•°æ®åº“ç»“æ„
4. {'âœ…' if sync_success else 'âŒ'} åŒæ­¥ç»“æ„åˆ°äº‘ç«¯

## æ–‡ä»¶ä½ç½®
- å¤‡ä»½ç›®å½•: {self.backup_dir}
- å¤‡ä»½æ–‡ä»¶: {backup_file or 'æ— '}
- ç»“æ„æ–‡ä»¶: {schema_file or 'æ— '}

## æ³¨æ„äº‹é¡¹
- äº‘ç«¯æ•°æ®åº“æ•°æ®å·²å¤‡ä»½ï¼Œå¦‚éœ€æ¢å¤è¯·ä½¿ç”¨å¤‡ä»½æ–‡ä»¶
- ç»“æ„åŒæ­¥å¯èƒ½ä¼šå½±å“ç°æœ‰æ•°æ®ï¼Œè¯·è°¨æ…æ“ä½œ
- å»ºè®®åœ¨åŒæ­¥åéªŒè¯åº”ç”¨åŠŸèƒ½æ˜¯å¦æ­£å¸¸
"""
        
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report_content)
        
        logger.info(f"åŒæ­¥æŠ¥å‘Šå·²ç”Ÿæˆ: {report_file}")
        return report_file
    
    def run(self):
        """æ‰§è¡Œå®Œæ•´çš„å¤‡ä»½å’ŒåŒæ­¥æµç¨‹"""
        logger.info("å¼€å§‹äº‘ç«¯æ•°æ®åº“å¤‡ä»½å’ŒåŒæ­¥æµç¨‹...")
        
        # 1. æµ‹è¯•è¿æ¥
        if not self.test_connection(self.local_db_url, "æœ¬åœ°"):
            return False
        
        if not self.test_connection(self.cloud_db_url, "äº‘ç«¯"):
            return False
        
        # 2. å¤‡ä»½äº‘ç«¯æ•°æ®åº“
        backup_file = self.backup_cloud_database()
        if not backup_file:
            logger.error("äº‘ç«¯æ•°æ®åº“å¤‡ä»½å¤±è´¥ï¼Œä¸­æ­¢åŒæ­¥")
            return False
        
        # 3. è·å–æœ¬åœ°ç»“æ„
        schema_file = self.get_local_schema()
        if not schema_file:
            logger.error("æœ¬åœ°æ•°æ®åº“ç»“æ„å¯¼å‡ºå¤±è´¥ï¼Œä¸­æ­¢åŒæ­¥")
            return False
        
        # 4. åŒæ­¥ç»“æ„åˆ°äº‘ç«¯
        sync_success = self.sync_schema_to_cloud(schema_file)
        
        # 5. ç”ŸæˆæŠ¥å‘Š
        report_file = self.generate_sync_report(backup_file, schema_file, sync_success)
        
        if sync_success:
            logger.info("ğŸ‰ äº‘ç«¯æ•°æ®åº“å¤‡ä»½å’ŒåŒæ­¥å®Œæˆ!")
            logger.info(f"ğŸ“‹ è¯¦ç»†æŠ¥å‘Š: {report_file}")
        else:
            logger.error("âŒ åŒæ­¥è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯ï¼Œè¯·æŸ¥çœ‹æ—¥å¿—")
        
        return sync_success

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description='äº‘ç«¯æ•°æ®åº“å¤‡ä»½å’Œæœ¬åœ°ç»“æ„åŒæ­¥')
    parser.add_argument('--dry-run', action='store_true', help='ä»…æµ‹è¯•è¿æ¥ï¼Œä¸æ‰§è¡Œå®é™…æ“ä½œ')
    args = parser.parse_args()
    
    sync_tool = CloudDatabaseSync()
    
    if args.dry_run:
        logger.info("æ‰§è¡Œè¯•è¿è¡Œæ¨¡å¼ï¼Œä»…æµ‹è¯•è¿æ¥...")
        local_ok = sync_tool.test_connection(sync_tool.local_db_url, "æœ¬åœ°")
        cloud_ok = sync_tool.test_connection(sync_tool.cloud_db_url, "äº‘ç«¯")
        
        if local_ok and cloud_ok:
            logger.info("âœ… æ‰€æœ‰æ•°æ®åº“è¿æ¥æ­£å¸¸ï¼Œå¯ä»¥æ‰§è¡ŒåŒæ­¥")
        else:
            logger.error("âŒ æ•°æ®åº“è¿æ¥å­˜åœ¨é—®é¢˜ï¼Œè¯·æ£€æŸ¥é…ç½®")
    else:
        sync_tool.run() 