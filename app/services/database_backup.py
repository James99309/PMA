#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ•°æ®åº“è‡ªåŠ¨å¤‡ä»½æœåŠ¡
æ”¯æŒå®šæ—¶å¤‡ä»½äº‘ç«¯PostgreSQLæ•°æ®åº“åˆ°æœ¬åœ°æˆ–äº‘å­˜å‚¨
"""

import os
import subprocess
import logging
from datetime import datetime, timedelta
import boto3
from botocore.exceptions import ClientError
import schedule
import time
import tempfile
import gzip
import shutil
import threading
from config import CLOUD_DB_URL
from urllib.parse import urlparse

logger = logging.getLogger(__name__)

class DatabaseBackupService:
    """æ•°æ®åº“å¤‡ä»½æœåŠ¡ç±»"""
    
    def __init__(self, config=None):
        self.config = config or {}
        self.backup_enabled = self.config.get('backup_enabled', True)
        self.backup_schedule = self.config.get('backup_schedule', '00:00')  # é»˜è®¤å‡Œæ™¨12ç‚¹
        self.retention_days = self.config.get('retention_days', 30)  # ä¿ç•™30å¤©
        self.backup_location = self.config.get('backup_location', './backups')
        self.cloud_storage = self.config.get('cloud_storage', {})
        
        # ç¡®ä¿å¤‡ä»½ç›®å½•å­˜åœ¨
        os.makedirs(self.backup_location, exist_ok=True)
        
        # æ ¹æ®ç¯å¢ƒåˆ¤æ–­ä½¿ç”¨å“ªä¸ªæ•°æ®åº“URL
        from config import LOCAL_DB_URL, CLOUD_DB_URL
        import os as env_os
        
        # å¦‚æœæ˜¯æœ¬åœ°ç¯å¢ƒï¼Œä½¿ç”¨æœ¬åœ°æ•°æ®åº“URL
        if env_os.environ.get('FLASK_ENV') == 'local':
            self.db_config = self._parse_database_url(LOCAL_DB_URL)
        else:
            self.db_config = self._parse_database_url(CLOUD_DB_URL)
        
    def _parse_database_url(self, database_url):
        """è§£ææ•°æ®åº“URL"""
        parsed = urlparse(database_url)
        return {
            'host': parsed.hostname,
            'port': parsed.port or 5432,
            'username': parsed.username,
            'password': parsed.password,
            'database': parsed.path.strip('/')
        }
    
    def create_backup(self, backup_type='full'):
        """åˆ›å»ºæ•°æ®åº“å¤‡ä»½"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        backup_filename = f"cloud_backup_{backup_type}_{timestamp}.sql"
        backup_path = os.path.join(self.backup_location, backup_filename)
        
        try:
            logger.info(f"å¼€å§‹åˆ›å»ºæ•°æ®åº“å¤‡ä»½: {backup_filename}")
            
            # æ„å»ºpg_dumpå‘½ä»¤
            cmd = [
                'pg_dump',
                '--host', self.db_config['host'],
                '--port', str(self.db_config['port']),
                '--username', self.db_config['username'],
                '--no-password',
                '--format', 'plain',
                '--clean',
                '--create',
                '--encoding', 'UTF8'
            ]
            
            # æ ¹æ®å¤‡ä»½ç±»å‹æ·»åŠ é€‰é¡¹
            if backup_type == 'schema_only':
                cmd.append('--schema-only')
            elif backup_type == 'data_only':
                cmd.append('--data-only')
            
            cmd.append(self.db_config['database'])
            
            # è®¾ç½®ç¯å¢ƒå˜é‡
            env = os.environ.copy()
            env['PGPASSWORD'] = self.db_config['password']
            
            # æ‰§è¡Œå¤‡ä»½å‘½ä»¤
            with open(backup_path, 'w') as f:
                result = subprocess.run(
                    cmd,
                    stdout=f,
                    stderr=subprocess.PIPE,
                    env=env,
                    text=True
                )
            
            if result.returncode == 0:
                file_size = os.path.getsize(backup_path) / (1024 * 1024)  # MB
                logger.info(f"âœ… å¤‡ä»½åˆ›å»ºæˆåŠŸ: {backup_filename} ({file_size:.2f} MB)")
                
                # ä¸Šä¼ åˆ°äº‘å­˜å‚¨ï¼ˆå¦‚æœé…ç½®äº†ï¼‰
                if self.cloud_storage.get('enabled'):
                    self._upload_to_cloud_storage(backup_path, backup_filename)
                
                return backup_path
            else:
                logger.error(f"âŒ å¤‡ä»½åˆ›å»ºå¤±è´¥: {result.stderr}")
                if os.path.exists(backup_path):
                    os.remove(backup_path)
                return None
                
        except Exception as e:
            logger.error(f"âŒ å¤‡ä»½è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")
            if os.path.exists(backup_path):
                os.remove(backup_path)
            return None
    
    def _upload_to_cloud_storage(self, file_path, filename):
        """ä¸Šä¼ å¤‡ä»½åˆ°äº‘å­˜å‚¨"""
        storage_type = self.cloud_storage.get('type', 'aws_s3')
        
        if storage_type == 'aws_s3':
            self._upload_to_s3(file_path, filename)
        elif storage_type == 'alibaba_oss':
            self._upload_to_oss(file_path, filename)
        
    def _upload_to_s3(self, file_path, filename):
        """ä¸Šä¼ åˆ°AWS S3"""
        try:
            s3_config = self.cloud_storage.get('aws_s3', {})
            s3_client = boto3.client(
                's3',
                aws_access_key_id=s3_config.get('access_key'),
                aws_secret_access_key=s3_config.get('secret_key'),
                region_name=s3_config.get('region', 'us-east-1')
            )
            
            bucket_name = s3_config.get('bucket_name')
            s3_key = f"database_backups/{filename}"
            
            s3_client.upload_file(file_path, bucket_name, s3_key)
            logger.info(f"âœ… å¤‡ä»½å·²ä¸Šä¼ åˆ°S3: s3://{bucket_name}/{s3_key}")
            
        except Exception as e:
            logger.error(f"âŒ S3ä¸Šä¼ å¤±è´¥: {str(e)}")
    
    def _upload_to_oss(self, file_path, filename):
        """ä¸Šä¼ åˆ°é˜¿é‡Œäº‘OSS"""
        try:
            import oss2
            oss_config = self.cloud_storage.get('alibaba_oss', {})
            
            auth = oss2.Auth(
                oss_config.get('access_key_id'),
                oss_config.get('access_key_secret')
            )
            
            bucket = oss2.Bucket(
                auth,
                oss_config.get('endpoint'),
                oss_config.get('bucket_name')
            )
            
            oss_key = f"database_backups/{filename}"
            bucket.put_object_from_file(oss_key, file_path)
            logger.info(f"âœ… å¤‡ä»½å·²ä¸Šä¼ åˆ°OSS: {oss_key}")
            
        except Exception as e:
            logger.error(f"âŒ OSSä¸Šä¼ å¤±è´¥: {str(e)}")
    
    def cleanup_old_backups(self):
        """æ¸…ç†è¿‡æœŸå¤‡ä»½æ–‡ä»¶"""
        try:
            cutoff_date = datetime.now() - timedelta(days=self.retention_days)
            deleted_count = 0
            
            for filename in os.listdir(self.backup_location):
                if filename.startswith('cloud_backup_') and filename.endswith('.sql'):
                    file_path = os.path.join(self.backup_location, filename)
                    file_mtime = datetime.fromtimestamp(os.path.getmtime(file_path))
                    
                    if file_mtime < cutoff_date:
                        os.remove(file_path)
                        deleted_count += 1
                        logger.info(f"ğŸ—‘ï¸ åˆ é™¤è¿‡æœŸå¤‡ä»½: {filename}")
            
            if deleted_count > 0:
                logger.info(f"âœ… æ¸…ç†å®Œæˆï¼Œåˆ é™¤äº† {deleted_count} ä¸ªè¿‡æœŸå¤‡ä»½æ–‡ä»¶")
            else:
                logger.info("âœ… æ— è¿‡æœŸå¤‡ä»½æ–‡ä»¶éœ€è¦æ¸…ç†")
                
        except Exception as e:
            logger.error(f"âŒ æ¸…ç†è¿‡æœŸå¤‡ä»½æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
    
    def create_incremental_backup(self):
        """åˆ›å»ºå¢é‡å¤‡ä»½ï¼ˆä»…å¤‡ä»½æœ€è¿‘æ›´æ”¹çš„æ•°æ®ï¼‰"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        backup_filename = f"cloud_backup_incremental_{timestamp}.sql"
        backup_path = os.path.join(self.backup_location, backup_filename)
        
        try:
            logger.info(f"å¼€å§‹åˆ›å»ºå¢é‡å¤‡ä»½: {backup_filename}")
            
            # è·å–æœ€è¿‘24å°æ—¶çš„æ•°æ®æ›´æ”¹
            tables_with_timestamps = [
                'projects', 'quotations', 'users', 'companies',
                'products', 'approval_instance', 'approval_record'
            ]
            
            yesterday = (datetime.now() - timedelta(days=1)).strftime('%Y-%m-%d')
            
            cmd = [
                'pg_dump',
                '--host', self.db_config['host'],
                '--port', str(self.db_config['port']),
                '--username', self.db_config['username'],
                '--no-password',
                '--format', 'plain',
                '--data-only'
            ]
            
            # æ·»åŠ è¡¨è¿‡æ»¤æ¡ä»¶
            for table in tables_with_timestamps:
                cmd.extend(['--table', table])
            
            cmd.append(self.db_config['database'])
            
            env = os.environ.copy()
            env['PGPASSWORD'] = self.db_config['password']
            
            with open(backup_path, 'w') as f:
                result = subprocess.run(cmd, stdout=f, stderr=subprocess.PIPE, env=env, text=True)
            
            if result.returncode == 0:
                file_size = os.path.getsize(backup_path) / (1024 * 1024)
                logger.info(f"âœ… å¢é‡å¤‡ä»½åˆ›å»ºæˆåŠŸ: {backup_filename} ({file_size:.2f} MB)")
                return backup_path
            else:
                logger.error(f"âŒ å¢é‡å¤‡ä»½åˆ›å»ºå¤±è´¥: {result.stderr}")
                if os.path.exists(backup_path):
                    os.remove(backup_path)
                return None
                
        except Exception as e:
            logger.error(f"âŒ å¢é‡å¤‡ä»½è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")
            return None
    
    def get_backup_status(self):
        """è·å–å¤‡ä»½çŠ¶æ€ä¿¡æ¯"""
        backup_files = []
        total_size = 0
        
        try:
            for filename in os.listdir(self.backup_location):
                if filename.startswith('cloud_backup_') and filename.endswith('.sql'):
                    file_path = os.path.join(self.backup_location, filename)
                    file_stat = os.stat(file_path)
                    file_size = file_stat.st_size
                    total_size += file_size
                    
                    backup_files.append({
                        'filename': filename,
                        'size': file_size,
                        'size_mb': file_size / (1024 * 1024),
                        'created_at': datetime.fromtimestamp(file_stat.st_mtime),
                        'age_days': (datetime.now() - datetime.fromtimestamp(file_stat.st_mtime)).days
                    })
            
            backup_files.sort(key=lambda x: x['created_at'], reverse=True)
            
            return {
                'backup_count': len(backup_files),
                'total_size_mb': total_size / (1024 * 1024),
                'latest_backup': backup_files[0] if backup_files else None,
                'backups': backup_files,
                'retention_days': self.retention_days,
                'backup_location': self.backup_location
            }
            
        except Exception as e:
            logger.error(f"âŒ è·å–å¤‡ä»½çŠ¶æ€æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
            return None
    
    def schedule_backups(self):
        """è®¾ç½®å®šæ—¶å¤‡ä»½"""
        if not self.backup_enabled:
            logger.info("å¤‡ä»½åŠŸèƒ½å·²ç¦ç”¨")
            return
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯æœ¬åœ°ç¯å¢ƒï¼ˆé€šè¿‡æ•°æ®åº“URLåˆ¤æ–­ï¼‰
        if 'localhost' in self.db_config.get('host', ''):
            logger.info("æœ¬åœ°ç¯å¢ƒï¼šä»…å¯ç”¨æ‰‹åŠ¨å¤‡ä»½ï¼Œè·³è¿‡å®šæ—¶å¤‡ä»½è®¾ç½®")
            return
        
        logger.info(f"è®¾ç½®å®šæ—¶å¤‡ä»½: æ¯å¤© {self.backup_schedule}")
        
        # è®¾ç½®æ¯æ—¥å®Œæ•´å¤‡ä»½
        schedule.every().day.at(self.backup_schedule).do(self._daily_backup_job)
        
        # è®¾ç½®æ¯6å°æ—¶å¢é‡å¤‡ä»½
        schedule.every(6).hours.do(self._incremental_backup_job)
        
        # è®¾ç½®æ¯å‘¨æ¸…ç†è¿‡æœŸå¤‡ä»½
        schedule.every().sunday.at("02:00").do(self.cleanup_old_backups)
        
        # å¯åŠ¨è°ƒåº¦å™¨çº¿ç¨‹
        backup_thread = threading.Thread(target=self._run_scheduler, daemon=True)
        backup_thread.start()
        logger.info("âœ… å®šæ—¶å¤‡ä»½è°ƒåº¦å™¨å·²å¯åŠ¨")
    
    def _daily_backup_job(self):
        """æ¯æ—¥å¤‡ä»½ä»»åŠ¡"""
        logger.info("ğŸ•› æ‰§è¡Œå®šæ—¶å®Œæ•´å¤‡ä»½...")
        self.create_backup('full')
    
    def _incremental_backup_job(self):
        """å¢é‡å¤‡ä»½ä»»åŠ¡"""
        logger.info("ğŸ•• æ‰§è¡Œå®šæ—¶å¢é‡å¤‡ä»½...")
        self.create_incremental_backup()
    
    def _run_scheduler(self):
        """è¿è¡Œè°ƒåº¦å™¨"""
        while True:
            schedule.run_pending()
            time.sleep(60)  # æ¯åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡

    def create_backup_with_email(self, backup_type='full'):
        """åˆ›å»ºå¤‡ä»½å¹¶é€šè¿‡é‚®ä»¶å‘é€"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        backup_filename = f"pma_backup_{backup_type}_{timestamp}.sql"
        
        try:
            logger.info(f"ğŸ”„ å¼€å§‹åˆ›å»º{backup_type}å¤‡ä»½: {backup_filename}")
            
            # åˆ›å»ºä¸´æ—¶å¤‡ä»½æ–‡ä»¶
            with tempfile.NamedTemporaryFile(mode='w', suffix='.sql', delete=False) as temp_file:
                backup_path = temp_file.name
            
            # æ„å»ºpg_dumpå‘½ä»¤
            cmd = [
                'pg_dump',
                '--host', self.db_config['host'],
                '--port', str(self.db_config['port']),
                '--username', self.db_config['username'],
                '--no-password',
                '--format', 'plain',
                '--clean',
                '--create',
                '--encoding', 'UTF8'
            ]
            
            # æ ¹æ®å¤‡ä»½ç±»å‹æ·»åŠ é€‰é¡¹
            if backup_type == 'schema_only':
                cmd.append('--schema-only')
            elif backup_type == 'data_only':
                cmd.append('--data-only')
            
            cmd.append(self.db_config['database'])
            
            # è®¾ç½®ç¯å¢ƒå˜é‡
            env = os.environ.copy()
            env['PGPASSWORD'] = self.db_config['password']
            
            # æ‰§è¡Œå¤‡ä»½
            with open(backup_path, 'w') as f:
                result = subprocess.run(cmd, stdout=f, stderr=subprocess.PIPE, env=env, text=True)
            
            if result.returncode != 0:
                logger.error(f"âŒ å¤‡ä»½åˆ›å»ºå¤±è´¥: {result.stderr}")
                os.remove(backup_path)
                return None
            
            # è·å–åŸå§‹æ–‡ä»¶å¤§å°
            original_size = os.path.getsize(backup_path)
            original_size_mb = original_size / (1024 * 1024)
            
            logger.info(f"âœ… åŸå§‹å¤‡ä»½æ–‡ä»¶: {original_size_mb:.2f} MB")
            
            # å‹ç¼©å¤‡ä»½æ–‡ä»¶
            compressed_path = backup_path + '.gz'
            with open(backup_path, 'rb') as f_in:
                with gzip.open(compressed_path, 'wb') as f_out:
                    f_out.writelines(f_in)
            
            compressed_size = os.path.getsize(compressed_path)
            compressed_size_mb = compressed_size / (1024 * 1024)
            compression_ratio = compressed_size / original_size if original_size > 0 else 0
            
            logger.info(f"âœ… å‹ç¼©å¤‡ä»½æ–‡ä»¶: {compressed_size_mb:.2f} MB (å‹ç¼©æ¯”: {compression_ratio:.1%})")
            
            # åˆ é™¤åŸå§‹æ–‡ä»¶ï¼Œä½¿ç”¨å‹ç¼©æ–‡ä»¶
            os.remove(backup_path)
            backup_path = compressed_path
            backup_filename = backup_filename + '.gz'
            
            # æ£€æŸ¥æ˜¯å¦é€‚åˆé‚®ä»¶å‘é€
            max_email_size_mb = 20  # ä¿å®ˆçš„é‚®ä»¶é™„ä»¶å¤§å°é™åˆ¶
            
            if compressed_size_mb <= max_email_size_mb:
                # å‘é€é‚®ä»¶
                email_sent = self._send_backup_email(backup_path, backup_filename, compressed_size_mb, backup_type)
                if email_sent:
                    logger.info(f"ğŸ“§ å¤‡ä»½æ–‡ä»¶å·²é€šè¿‡é‚®ä»¶å‘é€")
                else:
                    logger.warning(f"âš ï¸ é‚®ä»¶å‘é€å¤±è´¥ï¼Œå¤‡ä»½æ–‡ä»¶å·²ä¿å­˜åˆ°æœ¬åœ°")
            else:
                logger.warning(f"âš ï¸ å¤‡ä»½æ–‡ä»¶è¿‡å¤§ ({compressed_size_mb:.2f} MB)ï¼Œè·³è¿‡é‚®ä»¶å‘é€")
            
            # ä¿å­˜åˆ°æœ¬åœ°å¤‡ä»½ç›®å½•
            local_backup_path = os.path.join(self.backup_location, backup_filename)
            os.makedirs(self.backup_location, exist_ok=True)
            
            # ç§»åŠ¨æ–‡ä»¶åˆ°å¤‡ä»½ç›®å½•
            import shutil
            shutil.move(backup_path, local_backup_path)
            
            logger.info(f"ğŸ’¾ å¤‡ä»½æ–‡ä»¶å·²ä¿å­˜: {local_backup_path}")
            
            return local_backup_path
            
        except Exception as e:
            logger.error(f"âŒ å¤‡ä»½è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            for temp_path in [backup_path, compressed_path]:
                if 'temp_path' in locals() and os.path.exists(temp_path):
                    os.remove(temp_path)
            return None

    def _send_backup_email(self, backup_path, filename, file_size_mb, backup_type):
        """é€šè¿‡é‚®ä»¶å‘é€å¤‡ä»½æ–‡ä»¶"""
        try:
            from flask_mail import Message
            from app import mail
            
            # å‡†å¤‡é‚®ä»¶å†…å®¹
            timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            
            subject = f'PMAæ•°æ®åº“è‡ªåŠ¨å¤‡ä»½ - {timestamp}'
            
            # è·å–æ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯
            db_stats = self._get_database_stats()
            
            body = f"""
PMAç³»ç»Ÿè‡ªåŠ¨å¤‡ä»½æŠ¥å‘Š

ğŸ“Š å¤‡ä»½ä¿¡æ¯:
- å¤‡ä»½æ—¶é—´: {timestamp}
- å¤‡ä»½ç±»å‹: {backup_type}
- å¤‡ä»½æ–‡ä»¶: {filename}
- æ–‡ä»¶å¤§å°: {file_size_mb:.2f} MB

ğŸ“ˆ æ•°æ®åº“ç»Ÿè®¡:
- æ•°æ®åº“å¤§å°: {db_stats.get('db_size', 'N/A')}
- æ€»è®°å½•æ•°: {db_stats.get('total_records', 'N/A'):,} æ¡
- ä¸»è¦æ•°æ®è¡¨:
  â€¢ æŠ¥ä»·æ˜ç»†: {db_stats.get('quotation_details', 0):,} æ¡
  â€¢ é¡¹ç›®è®°å½•: {db_stats.get('projects', 0):,} æ¡
  â€¢ å…¬å¸ä¿¡æ¯: {db_stats.get('companies', 0):,} æ¡

ğŸ”§ ç³»ç»Ÿä¿¡æ¯:
- ç¯å¢ƒ: ç”Ÿäº§ç¯å¢ƒ
- ç‰ˆæœ¬: 1.0.1
- å¹³å°: Render Cloud
- å¤‡ä»½ç­–ç•¥: æ¯æ—¥è‡ªåŠ¨å¤‡ä»½

ğŸ“‹ ä½¿ç”¨è¯´æ˜:
1. ä¸‹è½½é™„ä»¶ä¸­çš„å¤‡ä»½æ–‡ä»¶
2. è§£å‹ .gz æ–‡ä»¶å¾—åˆ° .sql æ–‡ä»¶
3. ä½¿ç”¨ psql å‘½ä»¤æ¢å¤æ•°æ®åº“:
   psql -h hostname -U username -d database < backup_file.sql

âš ï¸ é‡è¦æé†’:
- è¯·å¦¥å–„ä¿å­˜æ­¤å¤‡ä»½æ–‡ä»¶
- å»ºè®®å®šæœŸæµ‹è¯•å¤‡ä»½æ–‡ä»¶çš„å®Œæ•´æ€§
- å¦‚æœ‰é—®é¢˜è¯·åŠæ—¶è”ç³»ç³»ç»Ÿç®¡ç†å‘˜

---
PMAè‡ªåŠ¨å¤‡ä»½ç³»ç»Ÿ
æ­¤é‚®ä»¶ç”±ç³»ç»Ÿè‡ªåŠ¨å‘é€ï¼Œè¯·å‹¿å›å¤
            """.strip()
            
            # åˆ›å»ºé‚®ä»¶
            recipients = [
                'James.ni@evertacsolutions.com',
                'james98980566@gmail.com'
            ]
            
            msg = Message(
                subject=subject,
                recipients=recipients,
                body=body
            )
            
            # æ·»åŠ å¤‡ä»½æ–‡ä»¶ä½œä¸ºé™„ä»¶
            with open(backup_path, 'rb') as f:
                msg.attach(
                    filename,
                    'application/gzip',
                    f.read(),
                    'attachment'
                )
            
            # å‘é€é‚®ä»¶
            mail.send(msg)
            logger.info(f"âœ… å¤‡ä»½é‚®ä»¶å·²å‘é€åˆ°: {', '.join(recipients)}")
            return True
            
        except Exception as e:
            logger.error(f"âŒ é‚®ä»¶å‘é€å¤±è´¥: {str(e)}")
            return False

    def _get_database_stats(self):
        """è·å–æ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯"""
        try:
            import psycopg2
            
            conn = psycopg2.connect(
                host=self.db_config['host'],
                port=self.db_config['port'],
                database=self.db_config['database'],
                user=self.db_config['username'],
                password=self.db_config['password']
            )
            
            stats = {}
            
            with conn.cursor() as cursor:
                # è·å–æ•°æ®åº“å¤§å°
                cursor.execute("SELECT pg_size_pretty(pg_database_size(current_database()))")
                stats['db_size'] = cursor.fetchone()[0]
                
                # è·å–ä¸»è¦è¡¨çš„è®°å½•æ•°
                tables = ['quotation_details', 'projects', 'companies', 'quotations', 'contacts']
                total_records = 0
                
                for table in tables:
                    try:
                        cursor.execute(f"SELECT COUNT(*) FROM {table}")
                        count = cursor.fetchone()[0]
                        stats[table] = count
                        total_records += count
                    except:
                        stats[table] = 0
                
                stats['total_records'] = total_records
            
            conn.close()
            return stats
            
        except Exception as e:
            logger.warning(f"è·å–æ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {str(e)}")
            return {}

    def _daily_backup_job_with_email(self):
        """æ¯æ—¥å¤‡ä»½ä»»åŠ¡ï¼ˆå¸¦é‚®ä»¶å‘é€ï¼‰"""
        logger.info("ğŸ•› æ‰§è¡Œå®šæ—¶å®Œæ•´å¤‡ä»½ï¼ˆé‚®ä»¶å‘é€ï¼‰...")
        self.create_backup_with_email('full')

# å…¨å±€å¤‡ä»½æœåŠ¡å®ä¾‹
backup_service = None

def init_backup_service(app):
    """åˆå§‹åŒ–å¤‡ä»½æœåŠ¡"""
    global backup_service
    
    backup_config = {
        'backup_enabled': app.config.get('BACKUP_ENABLED', True),
        'backup_schedule': app.config.get('BACKUP_SCHEDULE', '00:00'),
        'retention_days': app.config.get('BACKUP_RETENTION_DAYS', 30),
        'backup_location': app.config.get('BACKUP_LOCATION', './backups'),
        'cloud_storage': app.config.get('CLOUD_STORAGE_CONFIG', {})
    }
    
    backup_service = DatabaseBackupService(backup_config)
    
    if backup_config['backup_enabled']:
        backup_service.schedule_backups()
        # æ£€æŸ¥ç¯å¢ƒç±»å‹å¹¶ç»™å‡ºç›¸åº”æç¤º
        if hasattr(app.config, 'ENVIRONMENT') and app.config.ENVIRONMENT == 'local':
            logger.info("âœ… æœ¬åœ°æ•°æ®åº“å¤‡ä»½æœåŠ¡å·²åˆå§‹åŒ–ï¼ˆä»…æ‰‹åŠ¨å¤‡ä»½ï¼‰")
        else:
            logger.info("âœ… æ•°æ®åº“å¤‡ä»½æœåŠ¡å·²åˆå§‹åŒ–")
    else:
        logger.info("âš ï¸ æ•°æ®åº“å¤‡ä»½æœåŠ¡å·²ç¦ç”¨")
    
    return backup_service

def get_backup_service():
    """è·å–å¤‡ä»½æœåŠ¡å®ä¾‹"""
    return backup_service 