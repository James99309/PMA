#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å®‰å…¨æ•°æ®åº“åŒæ­¥è§£å†³æ–¹æ¡ˆ

è¯¥è„šæœ¬æä¾›å®‰å…¨çš„æ•°æ®åº“åŒæ­¥åŠŸèƒ½ï¼Œç‰¹åˆ«å…³æ³¨çº¦æŸé—®é¢˜çš„é¢„é˜²å’Œå¤„ç†ï¼š
1. é¢„æ£€æŸ¥çº¦æŸå†²çª
2. åˆ†æ­¥éª¤å®‰å…¨è¿ç§»
3. å›æ»šæœºåˆ¶
4. è¯¦ç»†çš„é£é™©è¯„ä¼°

ç”¨æ³•:
python3 safe_db_sync_solution.py [é€‰é¡¹]

é€‰é¡¹:
--check-constraints: æ£€æŸ¥çº¦æŸå†²çªé£é™©
--safe-sync: æ‰§è¡Œå®‰å…¨åŒæ­¥
--dry-run: é¢„è§ˆæ¨¡å¼ï¼Œä¸æ‰§è¡Œå®é™…æ“ä½œ
--backup-first: å…ˆå¤‡ä»½å†åŒæ­¥
--fix-constraints: ä¿®å¤çº¦æŸé—®é¢˜
"""

import os
import sys
import logging
import argparse
import json
import datetime
import subprocess
from pathlib import Path
from sqlalchemy import create_engine, inspect, text, MetaData

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('safe_db_sync.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger('å®‰å…¨æ•°æ®åº“åŒæ­¥')

class SafeDatabaseSync:
    def __init__(self):
        self.local_db_url = os.environ.get('DATABASE_URL', 'postgresql://nijie@localhost:5432/pma_local')
        self.render_db_url = os.environ.get('RENDER_DATABASE_URL')
        
        # ç¡®ä¿URLæ ¼å¼æ­£ç¡®
        if self.render_db_url and self.render_db_url.startswith('postgres://'):
            self.render_db_url = self.render_db_url.replace('postgres://', 'postgresql://', 1)
        
        self.local_engine = None
        self.render_engine = None
        
        # é£é™©è¯„ä¼°ç»“æœ
        self.risk_assessment = {
            'high_risk': [],
            'medium_risk': [],
            'low_risk': [],
            'safe': []
        }
    
    def connect_databases(self):
        """è¿æ¥åˆ°æœ¬åœ°å’Œäº‘ç«¯æ•°æ®åº“"""
        try:
            logger.info("è¿æ¥æœ¬åœ°æ•°æ®åº“...")
            self.local_engine = create_engine(self.local_db_url)
            
            if not self.render_db_url:
                logger.error("æœªè®¾ç½®RENDER_DATABASE_URLç¯å¢ƒå˜é‡")
                logger.info("è¯·è®¾ç½®äº‘ç«¯æ•°æ®åº“URL:")
                logger.info("export RENDER_DATABASE_URL='ä½ çš„äº‘ç«¯æ•°æ®åº“URL'")
                return False
            
            logger.info("è¿æ¥äº‘ç«¯æ•°æ®åº“...")
            self.render_engine = create_engine(self.render_db_url)
            
            # æµ‹è¯•è¿æ¥
            with self.local_engine.connect() as conn:
                conn.execute(text("SELECT 1"))
            logger.info("âœ“ æœ¬åœ°æ•°æ®åº“è¿æ¥æˆåŠŸ")
            
            with self.render_engine.connect() as conn:
                conn.execute(text("SELECT 1"))
            logger.info("âœ“ äº‘ç«¯æ•°æ®åº“è¿æ¥æˆåŠŸ")
            
            return True
            
        except Exception as e:
            logger.error(f"æ•°æ®åº“è¿æ¥å¤±è´¥: {str(e)}")
            return False
    
    def check_constraint_risks(self):
        """æ£€æŸ¥çº¦æŸå†²çªé£é™©"""
        if not self.connect_databases():
            return False
        
        logger.info("å¼€å§‹çº¦æŸé£é™©è¯„ä¼°...")
        
        # è·å–æ•°æ®åº“ç»“æ„
        local_inspector = inspect(self.local_engine)
        render_inspector = inspect(self.render_engine)
        
        local_tables = local_inspector.get_table_names()
        render_tables = render_inspector.get_table_names()
        
        # æ£€æŸ¥æ¯ä¸ªè¡¨çš„çº¦æŸé£é™©
        for table_name in local_tables:
            if table_name not in render_tables:
                self.risk_assessment['medium_risk'].append({
                    'type': 'missing_table',
                    'table': table_name,
                    'risk': 'éœ€è¦åˆ›å»ºæ•´ä¸ªè¡¨ç»“æ„',
                    'solution': 'ä½¿ç”¨CREATE TABLEè¯­å¥'
                })
                continue
            
            # æ£€æŸ¥åˆ—å·®å¼‚
            local_columns = {col['name']: col for col in local_inspector.get_columns(table_name)}
            render_columns = {col['name']: col for col in render_inspector.get_columns(table_name)}
            
            # æ£€æŸ¥æ–°å¢åˆ—çš„é£é™©
            for col_name, col_info in local_columns.items():
                if col_name not in render_columns:
                    risk_level = self._assess_column_add_risk(table_name, col_name, col_info)
                    self.risk_assessment[risk_level].append({
                        'type': 'missing_column',
                        'table': table_name,
                        'column': col_name,
                        'info': col_info,
                        'risk': self._get_column_risk_description(col_info),
                        'solution': self._get_column_add_solution(table_name, col_name, col_info)
                    })
            
            # æ£€æŸ¥ä¸»é”®çº¦æŸ
            local_pk = local_inspector.get_pk_constraint(table_name)
            render_pk = render_inspector.get_pk_constraint(table_name)
            
            if local_pk != render_pk:
                self.risk_assessment['high_risk'].append({
                    'type': 'primary_key_diff',
                    'table': table_name,
                    'local': local_pk,
                    'render': render_pk,
                    'risk': 'ä¸»é”®çº¦æŸå·®å¼‚å¯èƒ½å¯¼è‡´æ•°æ®ä¸€è‡´æ€§é—®é¢˜',
                    'solution': 'éœ€è¦æ‰‹åŠ¨å¤„ç†ä¸»é”®å†²çª'
                })
            
            # æ£€æŸ¥å¤–é”®çº¦æŸ
            local_fks = local_inspector.get_foreign_keys(table_name)
            render_fks = render_inspector.get_foreign_keys(table_name)
            
            if len(local_fks) != len(render_fks):
                self.risk_assessment['medium_risk'].append({
                    'type': 'foreign_key_diff',
                    'table': table_name,
                    'risk': 'å¤–é”®çº¦æŸæ•°é‡ä¸åŒ¹é…',
                    'solution': 'é€ä¸ªæ£€æŸ¥å¤–é”®çº¦æŸ'
                })
        
        # æ˜¾ç¤ºé£é™©è¯„ä¼°ç»“æœ
        self._display_risk_assessment()
        
        return True
    
    def _assess_column_add_risk(self, table_name, col_name, col_info):
        """è¯„ä¼°æ·»åŠ åˆ—çš„é£é™©çº§åˆ«"""
        # æ£€æŸ¥æ˜¯å¦æœ‰æ•°æ®
        try:
            with self.render_engine.connect() as conn:
                result = conn.execute(text(f"SELECT COUNT(*) FROM {table_name}"))
                row_count = result.fetchone()[0]
            
            # å¦‚æœè¡¨ä¸ºç©ºï¼Œé£é™©è¾ƒä½
            if row_count == 0:
                return 'low_risk'
            
            # å¦‚æœæ˜¯NOT NULLä¸”æ²¡æœ‰é»˜è®¤å€¼ï¼Œé£é™©è¾ƒé«˜
            if not col_info['nullable'] and col_info['default'] is None:
                return 'high_risk'
            
            # å¦‚æœæœ‰é»˜è®¤å€¼æˆ–å…è®¸NULLï¼Œé£é™©ä¸­ç­‰
            return 'medium_risk'
            
        except Exception:
            return 'medium_risk'
    
    def _get_column_risk_description(self, col_info):
        """è·å–åˆ—é£é™©æè¿°"""
        if not col_info['nullable'] and col_info['default'] is None:
            return 'NOT NULLåˆ—ä¸”æ— é»˜è®¤å€¼ï¼Œå¯èƒ½å¯¼è‡´ç°æœ‰æ•°æ®çº¦æŸå¤±è´¥'
        elif col_info['default']:
            return 'æœ‰é»˜è®¤å€¼ï¼Œç›¸å¯¹å®‰å…¨'
        else:
            return 'å…è®¸NULLï¼Œé£é™©è¾ƒä½'
    
    def _get_column_add_solution(self, table_name, col_name, col_info):
        """è·å–æ·»åŠ åˆ—çš„è§£å†³æ–¹æ¡ˆ"""
        col_type = col_info['type']
        
        if not col_info['nullable'] and col_info['default'] is None:
            # éœ€è¦å…ˆæ·»åŠ ä¸ºNULLï¼Œç„¶åè®¾ç½®é»˜è®¤å€¼ï¼Œæœ€åè®¾ä¸ºNOT NULL
            return f"""
            -- åˆ†æ­¥éª¤å®‰å…¨æ·»åŠ 
            ALTER TABLE {table_name} ADD COLUMN {col_name} {col_type} NULL;
            UPDATE {table_name} SET {col_name} = 'é€‚å½“çš„é»˜è®¤å€¼' WHERE {col_name} IS NULL;
            ALTER TABLE {table_name} ALTER COLUMN {col_name} SET NOT NULL;
            """
        else:
            nullable = "NULL" if col_info['nullable'] else "NOT NULL"
            default = f"DEFAULT {col_info['default']}" if col_info['default'] else ""
            return f"ALTER TABLE {table_name} ADD COLUMN IF NOT EXISTS {col_name} {col_type} {nullable} {default};"
    
    def _display_risk_assessment(self):
        """æ˜¾ç¤ºé£é™©è¯„ä¼°ç»“æœ"""
        logger.info("=== çº¦æŸé£é™©è¯„ä¼°ç»“æœ ===")
        
        total_issues = (len(self.risk_assessment['high_risk']) + 
                       len(self.risk_assessment['medium_risk']) + 
                       len(self.risk_assessment['low_risk']))
        
        if total_issues == 0:
            logger.info("âœ… æœªå‘ç°çº¦æŸé£é™©ï¼Œå¯ä»¥å®‰å…¨åŒæ­¥")
            return
        
        logger.warning(f"âš ï¸ å‘ç° {total_issues} ä¸ªæ½œåœ¨é£é™©")
        
        # é«˜é£é™©
        if self.risk_assessment['high_risk']:
            logger.error(f"ğŸ”´ é«˜é£é™©é¡¹ç›® ({len(self.risk_assessment['high_risk'])}ä¸ª):")
            for item in self.risk_assessment['high_risk']:
                logger.error(f"  - {item['type']}: {item.get('table', '')} - {item['risk']}")
        
        # ä¸­ç­‰é£é™©
        if self.risk_assessment['medium_risk']:
            logger.warning(f"ğŸŸ¡ ä¸­ç­‰é£é™©é¡¹ç›® ({len(self.risk_assessment['medium_risk'])}ä¸ª):")
            for item in self.risk_assessment['medium_risk']:
                logger.warning(f"  - {item['type']}: {item.get('table', '')} - {item['risk']}")
        
        # ä½é£é™©
        if self.risk_assessment['low_risk']:
            logger.info(f"ğŸŸ¢ ä½é£é™©é¡¹ç›® ({len(self.risk_assessment['low_risk'])}ä¸ª):")
            for item in self.risk_assessment['low_risk']:
                logger.info(f"  - {item['type']}: {item.get('table', '')} - {item['risk']}")
    
    def generate_safe_migration_sql(self):
        """ç”Ÿæˆå®‰å…¨çš„è¿ç§»SQL"""
        timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # é«˜é£é™©æ“ä½œå•ç‹¬æ–‡ä»¶
        high_risk_file = f"high_risk_migration_{timestamp}.sql"
        medium_risk_file = f"medium_risk_migration_{timestamp}.sql"
        low_risk_file = f"low_risk_migration_{timestamp}.sql"
        
        # ç”Ÿæˆé«˜é£é™©SQL
        if self.risk_assessment['high_risk']:
            with open(high_risk_file, 'w', encoding='utf-8') as f:
                f.write("-- é«˜é£é™©æ•°æ®åº“è¿ç§» - è¯·æ‰‹åŠ¨æ£€æŸ¥å’Œæ‰§è¡Œ\n")
                f.write(f"-- ç”Ÿæˆæ—¶é—´: {datetime.datetime.now()}\n\n")
                f.write("-- âš ï¸ è­¦å‘Šï¼šä»¥ä¸‹æ“ä½œæœ‰æ•°æ®é£é™©ï¼Œè¯·åœ¨æµ‹è¯•ç¯å¢ƒéªŒè¯åå†æ‰§è¡Œ\n\n")
                
                for item in self.risk_assessment['high_risk']:
                    f.write(f"-- {item['type']}: {item.get('table', '')}\n")
                    f.write(f"-- é£é™©: {item['risk']}\n")
                    f.write(f"-- è§£å†³æ–¹æ¡ˆ:\n")
                    f.write(f"{item['solution']}\n\n")
        
        # ç”Ÿæˆä¸­ç­‰é£é™©SQL
        if self.risk_assessment['medium_risk']:
            with open(medium_risk_file, 'w', encoding='utf-8') as f:
                f.write("-- ä¸­ç­‰é£é™©æ•°æ®åº“è¿ç§»\n")
                f.write(f"-- ç”Ÿæˆæ—¶é—´: {datetime.datetime.now()}\n\n")
                f.write("BEGIN;\n\n")
                
                for item in self.risk_assessment['medium_risk']:
                    if item['type'] == 'missing_column':
                        f.write(f"-- æ·»åŠ åˆ—: {item['table']}.{item['column']}\n")
                        f.write(f"{item['solution']}\n\n")
                
                f.write("COMMIT;\n")
        
        # ç”Ÿæˆä½é£é™©SQL
        if self.risk_assessment['low_risk']:
            with open(low_risk_file, 'w', encoding='utf-8') as f:
                f.write("-- ä½é£é™©æ•°æ®åº“è¿ç§» - å¯ä»¥å®‰å…¨æ‰§è¡Œ\n")
                f.write(f"-- ç”Ÿæˆæ—¶é—´: {datetime.datetime.now()}\n\n")
                f.write("BEGIN;\n\n")
                
                for item in self.risk_assessment['low_risk']:
                    if item['type'] == 'missing_column':
                        f.write(f"-- æ·»åŠ åˆ—: {item['table']}.{item['column']}\n")
                        f.write(f"{item['solution']}\n\n")
                
                f.write("COMMIT;\n")
        
        logger.info("å®‰å…¨è¿ç§»SQLæ–‡ä»¶å·²ç”Ÿæˆ:")
        if self.risk_assessment['high_risk']:
            logger.error(f"  ğŸ”´ é«˜é£é™©: {high_risk_file}")
        if self.risk_assessment['medium_risk']:
            logger.warning(f"  ğŸŸ¡ ä¸­ç­‰é£é™©: {medium_risk_file}")
        if self.risk_assessment['low_risk']:
            logger.info(f"  ğŸŸ¢ ä½é£é™©: {low_risk_file}")
    
    def backup_render_database(self):
        """å¤‡ä»½äº‘ç«¯æ•°æ®åº“"""
        if not self.render_db_url:
            logger.error("æœªè®¾ç½®äº‘ç«¯æ•°æ®åº“URLï¼Œæ— æ³•å¤‡ä»½")
            return False
        
        timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')
        backup_file = f"render_backup_{timestamp}.sql"
        
        logger.info(f"å¤‡ä»½äº‘ç«¯æ•°æ®åº“åˆ°: {backup_file}")
        
        try:
            # ä»URLä¸­æå–è¿æ¥ä¿¡æ¯
            url_parts = self.render_db_url.replace('postgresql://', '').split('/')
            conn_parts = url_parts[0].split('@')
            
            user_pass = conn_parts[0].split(':')
            host_port = conn_parts[1].split(':')
            
            username = user_pass[0]
            password = user_pass[1] if len(user_pass) > 1 else ''
            host = host_port[0]
            port = host_port[1] if len(host_port) > 1 else '5432'
            dbname = url_parts[1].split('?')[0]
            
            # è®¾ç½®ç¯å¢ƒå˜é‡
            env = os.environ.copy()
            env['PGPASSWORD'] = password
            
            # æ‰§è¡Œpg_dump
            cmd = [
                'pg_dump',
                '-h', host,
                '-p', port,
                '-U', username,
                '-d', dbname,
                '--verbose',
                '--clean',
                '--if-exists',
                '-f', backup_file
            ]
            
            result = subprocess.run(cmd, env=env, capture_output=True, text=True)
            
            if result.returncode == 0:
                logger.info(f"âœ… äº‘ç«¯æ•°æ®åº“å¤‡ä»½æˆåŠŸ: {backup_file}")
                return backup_file
            else:
                logger.error(f"å¤‡ä»½å¤±è´¥: {result.stderr}")
                return None
                
        except Exception as e:
            logger.error(f"å¤‡ä»½è¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}")
            return None
    
    def execute_safe_sync(self, dry_run=False, backup_first=True):
        """æ‰§è¡Œå®‰å…¨åŒæ­¥"""
        if not self.connect_databases():
            return False
        
        logger.info("å¼€å§‹å®‰å…¨æ•°æ®åº“åŒæ­¥...")
        
        # 1. å¤‡ä»½
        if backup_first:
            backup_file = self.backup_render_database()
            if not backup_file:
                logger.error("å¤‡ä»½å¤±è´¥ï¼ŒåŒæ­¥ä¸­æ­¢")
                return False
        
        # 2. é£é™©è¯„ä¼°
        if not self.check_constraint_risks():
            return False
        
        # 3. æ£€æŸ¥æ˜¯å¦æœ‰é«˜é£é™©æ“ä½œ
        if self.risk_assessment['high_risk']:
            logger.error("âš ï¸ æ£€æµ‹åˆ°é«˜é£é™©æ“ä½œï¼Œä¸èƒ½è‡ªåŠ¨æ‰§è¡Œ")
            logger.error("è¯·æ‰‹åŠ¨æ£€æŸ¥å¹¶å¤„ç†é«˜é£é™©é¡¹ç›®åå†è¯•")
            self.generate_safe_migration_sql()
            return False
        
        # 4. ç”Ÿæˆè¿ç§»SQL
        self.generate_safe_migration_sql()
        
        # 5. æ‰§è¡Œä½é£é™©å’Œä¸­ç­‰é£é™©æ“ä½œ
        if dry_run:
            logger.info("ğŸ” é¢„è§ˆæ¨¡å¼ï¼šä»¥ä¸‹æ˜¯å°†è¦æ‰§è¡Œçš„æ“ä½œ")
            self._preview_operations()
            return True
        
        # 6. ç¡®è®¤æ‰§è¡Œ
        total_operations = len(self.risk_assessment['medium_risk']) + len(self.risk_assessment['low_risk'])
        if total_operations == 0:
            logger.info("âœ… æ•°æ®åº“å·²åŒæ­¥ï¼Œæ— éœ€æ“ä½œ")
            return True
        
        response = input(f"\nå°†æ‰§è¡Œ {total_operations} ä¸ªæ“ä½œï¼Œæ˜¯å¦ç»§ç»­ï¼Ÿ(y/N): ").lower()
        if response != 'y':
            logger.info("åŒæ­¥å·²å–æ¶ˆ")
            return False
        
        # 7. æ‰§è¡Œè¿ç§»
        success = self._execute_migrations()
        
        if success:
            logger.info("âœ… å®‰å…¨åŒæ­¥å®Œæˆ")
        else:
            logger.error("âŒ åŒæ­¥è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯")
        
        return success
    
    def _preview_operations(self):
        """é¢„è§ˆå°†è¦æ‰§è¡Œçš„æ“ä½œ"""
        for risk_level in ['low_risk', 'medium_risk']:
            if self.risk_assessment[risk_level]:
                logger.info(f"\n{risk_level.replace('_', ' ').title()}æ“ä½œ:")
                for item in self.risk_assessment[risk_level]:
                    if item['type'] == 'missing_column':
                        logger.info(f"  - æ·»åŠ åˆ—: {item['table']}.{item['column']}")
                    elif item['type'] == 'missing_table':
                        logger.info(f"  - åˆ›å»ºè¡¨: {item['table']}")
    
    def _execute_migrations(self):
        """æ‰§è¡Œè¿ç§»æ“ä½œ"""
        try:
            with self.render_engine.connect() as conn:
                trans = conn.begin()
                
                try:
                    executed_count = 0
                    
                    # æ‰§è¡Œä½é£é™©æ“ä½œ
                    for item in self.risk_assessment['low_risk']:
                        if item['type'] == 'missing_column':
                            sql = item['solution'].strip()
                            if sql and not sql.startswith('--'):
                                logger.info(f"æ‰§è¡Œ: {sql[:100]}...")
                                conn.execute(text(sql))
                                executed_count += 1
                    
                    # æ‰§è¡Œä¸­ç­‰é£é™©æ“ä½œ
                    for item in self.risk_assessment['medium_risk']:
                        if item['type'] == 'missing_column':
                            sql = item['solution'].strip()
                            if sql and not sql.startswith('--'):
                                logger.info(f"æ‰§è¡Œ: {sql[:100]}...")
                                conn.execute(text(sql))
                                executed_count += 1
                    
                    trans.commit()
                    logger.info(f"âœ… æˆåŠŸæ‰§è¡Œ {executed_count} ä¸ªæ“ä½œ")
                    return True
                    
                except Exception as e:
                    trans.rollback()
                    logger.error(f"æ‰§è¡Œå¤±è´¥ï¼Œå·²å›æ»š: {str(e)}")
                    return False
                    
        except Exception as e:
            logger.error(f"è¿æ¥æ•°æ®åº“å¤±è´¥: {str(e)}")
            return False

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='å®‰å…¨æ•°æ®åº“åŒæ­¥è§£å†³æ–¹æ¡ˆ')
    parser.add_argument('--check-constraints', action='store_true', help='æ£€æŸ¥çº¦æŸå†²çªé£é™©')
    parser.add_argument('--safe-sync', action='store_true', help='æ‰§è¡Œå®‰å…¨åŒæ­¥')
    parser.add_argument('--dry-run', action='store_true', help='é¢„è§ˆæ¨¡å¼ï¼Œä¸æ‰§è¡Œå®é™…æ“ä½œ')
    parser.add_argument('--backup-first', action='store_true', default=True, help='å…ˆå¤‡ä»½å†åŒæ­¥')
    parser.add_argument('--fix-constraints', action='store_true', help='ä¿®å¤çº¦æŸé—®é¢˜')
    
    args = parser.parse_args()
    
    # åˆ›å»ºåŒæ­¥å·¥å…·å®ä¾‹
    sync_tool = SafeDatabaseSync()
    
    try:
        if args.check_constraints:
            # ä»…æ£€æŸ¥çº¦æŸé£é™©
            success = sync_tool.check_constraint_risks()
            if success:
                sync_tool.generate_safe_migration_sql()
            return 0 if success else 1
        
        elif args.safe_sync:
            # æ‰§è¡Œå®‰å…¨åŒæ­¥
            success = sync_tool.execute_safe_sync(
                dry_run=args.dry_run,
                backup_first=args.backup_first
            )
            return 0 if success else 1
        
        else:
            # é»˜è®¤ï¼šæ£€æŸ¥çº¦æŸé£é™©
            logger.info("æœªæŒ‡å®šæ“ä½œï¼Œé»˜è®¤æ‰§è¡Œçº¦æŸé£é™©æ£€æŸ¥")
            logger.info("ä½¿ç”¨ --help æŸ¥çœ‹æ‰€æœ‰é€‰é¡¹")
            success = sync_tool.check_constraint_risks()
            if success:
                sync_tool.generate_safe_migration_sql()
            return 0 if success else 1
    
    except KeyboardInterrupt:
        logger.info("æ“ä½œè¢«ç”¨æˆ·ä¸­æ–­")
        return 1
    except Exception as e:
        logger.error(f"æ‰§è¡Œè¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}")
        return 1

if __name__ == "__main__":
    sys.exit(main()) 