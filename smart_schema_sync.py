#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ™ºèƒ½æ•°æ®åº“ç»“æ„åŒæ­¥è„šæœ¬
åŠŸèƒ½ï¼šåªåŒæ­¥æ–°å¢çš„è¡¨ã€å­—æ®µå’Œç´¢å¼•ï¼Œä¸ç ´åç°æœ‰ç»“æ„å’Œæ•°æ®
"""

import os
import sys
import psycopg2
import logging
import datetime

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger('æ™ºèƒ½åŒæ­¥')

class SmartSchemaSync:
    def __init__(self):
        self.timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')
        self.sync_dir = os.path.join(os.getcwd(), 'sync_logs')
        os.makedirs(self.sync_dir, exist_ok=True)
        
        # æ•°æ®åº“è¿æ¥ä¿¡æ¯
        self.cloud_db_url = "postgresql://pma_db_sp8d_user:LXNGJmR6bFrNecoaWbdbdzPpltIAd40w@dpg-d0b1gl1r0fns73d1jc1g-a.singapore-postgres.render.com/pma_db_sp8d"
        self.local_db_url = "postgresql://nijie@localhost:5432/pma_local"

    def get_db_connection(self, db_url):
        """è·å–æ•°æ®åº“è¿æ¥"""
        from urllib.parse import urlparse
        parsed = urlparse(db_url)
        return psycopg2.connect(
            host=parsed.hostname,
            port=parsed.port or 5432,
            user=parsed.username,
            password=parsed.password,
            dbname=parsed.path.lstrip('/')
        )

    def get_table_schemas(self, conn):
        """è·å–æ•°æ®åº“è¡¨ç»“æ„ä¿¡æ¯"""
        cursor = conn.cursor()
        
        # è·å–è¡¨ä¿¡æ¯
        cursor.execute("""
            SELECT table_name, table_type
            FROM information_schema.tables 
            WHERE table_schema = 'public'
            ORDER BY table_name;
        """)
        tables = {row[0]: row[1] for row in cursor.fetchall()}
        
        # è·å–åˆ—ä¿¡æ¯
        cursor.execute("""
            SELECT table_name, column_name, data_type, is_nullable, column_default
            FROM information_schema.columns 
            WHERE table_schema = 'public'
            ORDER BY table_name, ordinal_position;
        """)
        columns = {}
        for table, column, data_type, nullable, default in cursor.fetchall():
            if table not in columns:
                columns[table] = {}
            columns[table][column] = {
                'type': data_type,
                'nullable': nullable,
                'default': default
            }
        
        # è·å–ç´¢å¼•ä¿¡æ¯
        cursor.execute("""
            SELECT schemaname, tablename, indexname, indexdef
            FROM pg_indexes 
            WHERE schemaname = 'public'
            ORDER BY tablename, indexname;
        """)
        indexes = {}
        for schema, table, index_name, index_def in cursor.fetchall():
            if table not in indexes:
                indexes[table] = {}
            indexes[table][index_name] = index_def
        
        cursor.close()
        return tables, columns, indexes

    def compare_schemas(self):
        """æ¯”è¾ƒæœ¬åœ°å’Œäº‘ç«¯æ•°æ®åº“ç»“æ„"""
        logger.info("æ¯”è¾ƒæœ¬åœ°å’Œäº‘ç«¯æ•°æ®åº“ç»“æ„...")
        
        # è¿æ¥æ•°æ®åº“
        local_conn = self.get_db_connection(self.local_db_url)
        cloud_conn = self.get_db_connection(self.cloud_db_url)
        
        # è·å–ç»“æ„ä¿¡æ¯
        local_tables, local_columns, local_indexes = self.get_table_schemas(local_conn)
        cloud_tables, cloud_columns, cloud_indexes = self.get_table_schemas(cloud_conn)
        
        # æ‰¾å‡ºå·®å¼‚
        new_tables = set(local_tables.keys()) - set(cloud_tables.keys())
        new_columns = {}
        
        for table in local_columns:
            if table in cloud_columns:
                new_cols = set(local_columns[table].keys()) - set(cloud_columns[table].keys())
                if new_cols:
                    new_columns[table] = {col: local_columns[table][col] for col in new_cols}
        
        logger.info(f"å‘ç° {len(new_tables)} ä¸ªæ–°è¡¨")
        logger.info(f"å‘ç° {sum(len(cols) for cols in new_columns.values())} ä¸ªæ–°å­—æ®µ")
        
        for table in new_tables:
            logger.info(f"  æ–°è¡¨: {table}")
        
        for table, cols in new_columns.items():
            for col in cols:
                logger.info(f"  æ–°å­—æ®µ: {table}.{col}")
        
        local_conn.close()
        cloud_conn.close()
        
        return new_tables, new_columns

    def generate_migration_sql(self, new_tables, new_columns):
        """ç”Ÿæˆè¿ç§»SQL"""
        logger.info("ç”Ÿæˆè¿ç§»SQL...")
        
        migration_sql = []
        
        # ä¸ºæ–°è¡¨ç”ŸæˆCREATE TABLEè¯­å¥
        if new_tables:
            local_conn = self.get_db_connection(self.local_db_url)
            cursor = local_conn.cursor()
            
            for table in new_tables:
                # è·å–è¡¨çš„å®Œæ•´åˆ›å»ºè¯­å¥
                cursor.execute(f"""
                    SELECT column_name, data_type, is_nullable, column_default
                    FROM information_schema.columns 
                    WHERE table_schema = 'public' AND table_name = '{table}'
                    ORDER BY ordinal_position;
                """)
                columns = cursor.fetchall()
                
                if columns:
                    create_sql = f"CREATE TABLE IF NOT EXISTS {table} (\n"
                    col_defs = []
                    
                    for col_name, data_type, nullable, default in columns:
                        col_def = f"    {col_name} {data_type}"
                        if nullable == 'NO':
                            col_def += " NOT NULL"
                        if default:
                            col_def += f" DEFAULT {default}"
                        col_defs.append(col_def)
                    
                    create_sql += ",\n".join(col_defs)
                    create_sql += "\n);"
                    
                    migration_sql.append(create_sql)
                    logger.info(f"ç”Ÿæˆæ–°è¡¨SQL: {table}")
            
            cursor.close()
            local_conn.close()
        
        # ä¸ºæ–°å­—æ®µç”ŸæˆALTER TABLEè¯­å¥
        for table, columns in new_columns.items():
            for col_name, col_info in columns.items():
                alter_sql = f"ALTER TABLE {table} ADD COLUMN IF NOT EXISTS {col_name} {col_info['type']}"
                if col_info['nullable'] == 'NO':
                    alter_sql += " NOT NULL"
                if col_info['default']:
                    alter_sql += f" DEFAULT {col_info['default']}"
                alter_sql += ";"
                
                migration_sql.append(alter_sql)
                logger.info(f"ç”Ÿæˆæ–°å­—æ®µSQL: {table}.{col_name}")
        
        return migration_sql

    def apply_migration(self, migration_sql):
        """åº”ç”¨è¿ç§»SQL"""
        if not migration_sql:
            logger.info("æ²¡æœ‰éœ€è¦è¿ç§»çš„ç»“æ„ï¼Œè·³è¿‡")
            return True
        
        logger.info(f"åº”ç”¨ {len(migration_sql)} ä¸ªè¿ç§»è¯­å¥...")
        
        cloud_conn = self.get_db_connection(self.cloud_db_url)
        cloud_conn.autocommit = False
        cursor = cloud_conn.cursor()
        
        try:
            for i, sql in enumerate(migration_sql, 1):
                logger.info(f"æ‰§è¡Œ ({i}/{len(migration_sql)}): {sql[:50]}...")
                cursor.execute(sql)
            
            cloud_conn.commit()
            logger.info("âœ… æ‰€æœ‰è¿ç§»è¯­å¥æ‰§è¡ŒæˆåŠŸ")
            
            cursor.close()
            cloud_conn.close()
            return True
            
        except Exception as e:
            logger.error(f"âŒ è¿ç§»å¤±è´¥: {e}")
            cloud_conn.rollback()
            cursor.close()
            cloud_conn.close()
            return False

    def verify_migration(self):
        """éªŒè¯è¿ç§»ç»“æœ"""
        logger.info("éªŒè¯è¿ç§»ç»“æœ...")
        
        try:
            cloud_conn = self.get_db_connection(self.cloud_db_url)
            cursor = cloud_conn.cursor()
            
            # è·å–æ€»è¡Œæ•°
            cursor.execute("""
                SELECT table_name
                FROM information_schema.tables 
                WHERE table_schema = 'public' AND table_type = 'BASE TABLE'
                ORDER BY table_name;
            """)
            tables = cursor.fetchall()
            
            total_rows = 0
            for (table_name,) in tables:
                try:
                    cursor.execute(f"SELECT COUNT(*) FROM {table_name};")
                    count = cursor.fetchone()[0]
                    total_rows += count
                except:
                    pass
            
            # è·å–è¡¨æ•°é‡
            cursor.execute("""
                SELECT COUNT(*) FROM information_schema.tables 
                WHERE table_schema = 'public' AND table_type = 'BASE TABLE'
            """)
            table_count = cursor.fetchone()[0]
            
            logger.info(f"è¿ç§»åç»Ÿè®¡: {table_count} ä¸ªè¡¨, {total_rows} è¡Œæ•°æ®")
            
            cursor.close()
            cloud_conn.close()
            
            return total_rows, table_count
            
        except Exception as e:
            logger.error(f"éªŒè¯å¤±è´¥: {e}")
            return 0, 0

    def save_migration_log(self, migration_sql, success, before_stats, after_stats):
        """ä¿å­˜è¿ç§»æ—¥å¿—"""
        log_file = os.path.join(self.sync_dir, f'migration_log_{self.timestamp}.md')
        
        status = "âœ… æˆåŠŸ" if success else "âŒ å¤±è´¥"
        
        log_content = f"""# æ•°æ®åº“æ™ºèƒ½è¿ç§»æ—¥å¿—

## è¿ç§»æ¦‚è¿°
- è¿ç§»æ—¶é—´: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
- è¿ç§»çŠ¶æ€: {status}
- è¿ç§»è¯­å¥æ•°é‡: {len(migration_sql)}

## æ•°æ®ç»Ÿè®¡
- è¿ç§»å‰: {before_stats[1]} ä¸ªè¡¨, {before_stats[0]} è¡Œæ•°æ®
- è¿ç§»å: {after_stats[1]} ä¸ªè¡¨, {after_stats[0]} è¡Œæ•°æ®
- æ–°å¢è¡¨: {after_stats[1] - before_stats[1]} ä¸ª
- æ•°æ®å˜åŒ–: {after_stats[0] - before_stats[0]} è¡Œ

## æ‰§è¡Œçš„SQLè¯­å¥
```sql
{chr(10).join(migration_sql)}
```

## ç»“è®º
{f'âœ… è¿ç§»æˆåŠŸå®Œæˆï¼Œæ‰€æœ‰æ–°ç»“æ„å·²åŒæ­¥åˆ°äº‘ç«¯' if success else 'âŒ è¿ç§»å¤±è´¥ï¼Œè¯·æ£€æŸ¥é”™è¯¯æ—¥å¿—'}
"""
        
        with open(log_file, 'w', encoding='utf-8') as f:
            f.write(log_content)
        
        logger.info(f"è¿ç§»æ—¥å¿—å·²ä¿å­˜: {log_file}")
        return log_file

    def run(self):
        """æ‰§è¡Œæ™ºèƒ½è¿ç§»"""
        logger.info("ğŸš€ å¼€å§‹æ™ºèƒ½æ•°æ®åº“ç»“æ„åŒæ­¥...")
        
        # è·å–è¿ç§»å‰ç»Ÿè®¡
        before_rows, before_tables = self.verify_migration()
        
        # æ¯”è¾ƒç»“æ„å·®å¼‚
        new_tables, new_columns = self.compare_schemas()
        
        if not new_tables and not new_columns:
            logger.info("âœ… æ•°æ®åº“ç»“æ„å·²æ˜¯æœ€æ–°ï¼Œæ— éœ€åŒæ­¥")
            return True
        
        # ç”Ÿæˆè¿ç§»SQL
        migration_sql = self.generate_migration_sql(new_tables, new_columns)
        
        if not migration_sql:
            logger.info("âœ… æ— éœ€è¦åº”ç”¨çš„è¿ç§»")
            return True
        
        # åº”ç”¨è¿ç§»
        success = self.apply_migration(migration_sql)
        
        # è·å–è¿ç§»åç»Ÿè®¡
        after_rows, after_tables = self.verify_migration()
        
        # ä¿å­˜æ—¥å¿—
        log_file = self.save_migration_log(migration_sql, success, 
                                         (before_rows, before_tables), 
                                         (after_rows, after_tables))
        
        if success:
            logger.info("ğŸ‰ æ™ºèƒ½è¿ç§»æˆåŠŸå®Œæˆ!")
            logger.info(f"ğŸ“Š ç»Ÿè®¡: è¡¨ {before_tables} -> {after_tables}, æ•°æ® {before_rows} -> {after_rows}")
            logger.info(f"ğŸ“‹ è¯¦ç»†æ—¥å¿—: {log_file}")
        else:
            logger.error("âŒ è¿ç§»å¤±è´¥ï¼Œè¯·æŸ¥çœ‹æ—¥å¿—")
        
        return success

if __name__ == "__main__":
    sync_tool = SmartSchemaSync()
    success = sync_tool.run()
    if success:
        logger.info("ğŸ¯ æ™ºèƒ½åŒæ­¥æ“ä½œæˆåŠŸå®Œæˆ!")
    else:
        logger.error("ğŸ’¥ æ™ºèƒ½åŒæ­¥æ“ä½œå¤±è´¥!")
