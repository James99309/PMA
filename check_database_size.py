#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ£€æŸ¥äº‘ç«¯æ•°æ®åº“å¤§å°å’Œé‚®ä»¶å¤‡ä»½å¯è¡Œæ€§åˆ†æ
"""

import psycopg2
import subprocess
import os
import gzip
import tempfile
from urllib.parse import urlparse
from config import CLOUD_DB_URL
from datetime import datetime
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class DatabaseSizeAnalyzer:
    """æ•°æ®åº“å¤§å°åˆ†æå™¨"""
    
    def __init__(self):
        self.db_config = self._parse_database_url(CLOUD_DB_URL)
        
    def _parse_database_url(self, database_url):
        """è§£ææ•°æ®åº“URL"""
        parsed = urlparse(database_url)
        return {
            'host': parsed.hostname,
            'port': parsed.port or 5432,
            'username': parsed.username,
            'password': parsed.password,
            'database': parsed.path.strip('/')
        }
    
    def get_database_size_info(self):
        """è·å–æ•°æ®åº“å¤§å°ä¿¡æ¯"""
        try:
            conn = psycopg2.connect(
                host=self.db_config['host'],
                port=self.db_config['port'],
                database=self.db_config['database'],
                user=self.db_config['username'],
                password=self.db_config['password']
            )
            
            with conn.cursor() as cursor:
                # è·å–æ•°æ®åº“æ€»å¤§å°
                cursor.execute("""
                    SELECT pg_size_pretty(pg_database_size(current_database())) as db_size,
                           pg_database_size(current_database()) as db_size_bytes
                """)
                db_size_pretty, db_size_bytes = cursor.fetchone()
                
                # è·å–å„è¡¨çš„å¤§å°
                cursor.execute("""
                    SELECT 
                        schemaname,
                        tablename,
                        pg_size_pretty(pg_total_relation_size(schemaname||'.'||tablename)) as size,
                        pg_total_relation_size(schemaname||'.'||tablename) as size_bytes,
                        pg_size_pretty(pg_relation_size(schemaname||'.'||tablename)) as table_size,
                        pg_size_pretty(pg_total_relation_size(schemaname||'.'||tablename) - pg_relation_size(schemaname||'.'||tablename)) as index_size
                    FROM pg_tables 
                    WHERE schemaname = 'public'
                    ORDER BY pg_total_relation_size(schemaname||'.'||tablename) DESC
                """)
                
                table_sizes = cursor.fetchall()
                
                # è·å–è®°å½•æ•°ç»Ÿè®¡
                cursor.execute("""
                    SELECT table_name 
                    FROM information_schema.tables 
                    WHERE table_schema = 'public' 
                    AND table_type = 'BASE TABLE'
                    ORDER BY table_name
                """)
                
                tables = [row[0] for row in cursor.fetchall()]
                
                record_counts = {}
                total_records = 0
                
                for table_name in tables:
                    try:
                        cursor.execute(f"SELECT COUNT(*) FROM {table_name}")
                        count = cursor.fetchone()[0]
                        record_counts[table_name] = count
                        total_records += count
                    except Exception as e:
                        record_counts[table_name] = f"Error: {str(e)}"
            
            conn.close()
            
            return {
                'db_size_pretty': db_size_pretty,
                'db_size_bytes': db_size_bytes,
                'db_size_mb': db_size_bytes / (1024 * 1024),
                'table_sizes': table_sizes,
                'record_counts': record_counts,
                'total_records': total_records
            }
            
        except Exception as e:
            logger.error(f"è·å–æ•°æ®åº“å¤§å°ä¿¡æ¯å¤±è´¥: {str(e)}")
            return None
    
    def create_test_backup(self):
        """åˆ›å»ºæµ‹è¯•å¤‡ä»½ä»¥è¯„ä¼°å®é™…å¤§å°"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        try:
            # åˆ›å»ºä¸´æ—¶å¤‡ä»½æ–‡ä»¶
            with tempfile.NamedTemporaryFile(mode='w', suffix='.sql', delete=False) as temp_file:
                backup_path = temp_file.name
            
            logger.info("åˆ›å»ºæµ‹è¯•å¤‡ä»½...")
            
            # æ„å»ºpg_dumpå‘½ä»¤
            cmd = [
                'pg_dump',
                '--host', self.db_config['host'],
                '--port', str(self.db_config['port']),
                '--username', self.db_config['username'],
                '--no-password',
                '--format', 'plain',
                '--clean',
                '--create',
                '--encoding', 'UTF8',
                self.db_config['database']
            ]
            
            env = os.environ.copy()
            env['PGPASSWORD'] = self.db_config['password']
            
            # æ‰§è¡Œå¤‡ä»½
            with open(backup_path, 'w') as f:
                result = subprocess.run(cmd, stdout=f, stderr=subprocess.PIPE, env=env, text=True)
            
            if result.returncode != 0:
                logger.error(f"å¤‡ä»½åˆ›å»ºå¤±è´¥: {result.stderr}")
                return None
            
            # è·å–åŸå§‹å¤‡ä»½æ–‡ä»¶å¤§å°
            original_size = os.path.getsize(backup_path)
            
            # åˆ›å»ºå‹ç¼©ç‰ˆæœ¬
            compressed_path = backup_path + '.gz'
            with open(backup_path, 'rb') as f_in:
                with gzip.open(compressed_path, 'wb') as f_out:
                    f_out.writelines(f_in)
            
            compressed_size = os.path.getsize(compressed_path)
            
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            os.remove(backup_path)
            os.remove(compressed_path)
            
            return {
                'original_size_bytes': original_size,
                'original_size_mb': original_size / (1024 * 1024),
                'compressed_size_bytes': compressed_size,
                'compressed_size_mb': compressed_size / (1024 * 1024),
                'compression_ratio': compressed_size / original_size if original_size > 0 else 0
            }
            
        except Exception as e:
            logger.error(f"åˆ›å»ºæµ‹è¯•å¤‡ä»½å¤±è´¥: {str(e)}")
            return None
    
    def analyze_email_backup_feasibility(self, db_info, backup_info):
        """åˆ†æé‚®ä»¶å¤‡ä»½å¯è¡Œæ€§"""
        
        # é‚®ä»¶é™„ä»¶å¤§å°é™åˆ¶
        email_limits = {
            'gmail': 25,      # Gmail: 25MB
            'outlook': 20,    # Outlook: 20MB
            'yahoo': 25,      # Yahoo: 25MB
            'general': 20     # ä¸€èˆ¬é‚®ä»¶æœåŠ¡å™¨: 20MB
        }
        
        analysis = {
            'database_size_mb': db_info['db_size_mb'],
            'backup_original_mb': backup_info['original_size_mb'] if backup_info else 0,
            'backup_compressed_mb': backup_info['compressed_size_mb'] if backup_info else 0,
            'compression_ratio': backup_info['compression_ratio'] if backup_info else 0,
            'email_feasible': {},
            'recommendations': []
        }
        
        if backup_info:
            compressed_size_mb = backup_info['compressed_size_mb']
            
            # æ£€æŸ¥å„é‚®ä»¶æœåŠ¡çš„å¯è¡Œæ€§
            for service, limit in email_limits.items():
                analysis['email_feasible'][service] = {
                    'limit_mb': limit,
                    'feasible': compressed_size_mb <= limit,
                    'size_ratio': compressed_size_mb / limit
                }
            
            # ç”Ÿæˆå»ºè®®
            if compressed_size_mb <= 20:
                analysis['recommendations'].append("âœ… é‚®ä»¶å¤‡ä»½å®Œå…¨å¯è¡Œï¼Œæ‰€æœ‰ä¸»æµé‚®ä»¶æœåŠ¡éƒ½æ”¯æŒ")
            elif compressed_size_mb <= 25:
                analysis['recommendations'].append("âš ï¸ é‚®ä»¶å¤‡ä»½åŸºæœ¬å¯è¡Œï¼Œä½†éœ€è¦é€‰æ‹©æ”¯æŒ25MBçš„é‚®ä»¶æœåŠ¡ï¼ˆå¦‚Gmailï¼‰")
            else:
                analysis['recommendations'].append("âŒ é‚®ä»¶å¤‡ä»½ä¸å¯è¡Œï¼Œéœ€è¦è€ƒè™‘å…¶ä»–æ–¹æ¡ˆ")
                analysis['recommendations'].append("ğŸ’¡ å»ºè®®ä½¿ç”¨äº‘å­˜å‚¨ï¼ˆAWS S3ã€é˜¿é‡Œäº‘OSSï¼‰æˆ–GitHubç§æœ‰ä»“åº“")
        
        return analysis
    
    def generate_backup_strategy_report(self, db_info, backup_info, analysis):
        """ç”Ÿæˆå¤‡ä»½ç­–ç•¥æŠ¥å‘Š"""
        
        report = f"""
# ğŸ“Š PMAæ•°æ®åº“å¤‡ä»½ç­–ç•¥åˆ†ææŠ¥å‘Š

**åˆ†ææ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

## ğŸ—„ï¸ æ•°æ®åº“è§„æ¨¡åˆ†æ

### æ€»ä½“è§„æ¨¡
- **æ•°æ®åº“å¤§å°**: {db_info['db_size_pretty']} ({db_info['db_size_mb']:.2f} MB)
- **æ€»è®°å½•æ•°**: {db_info['total_records']:,} æ¡
- **è¡¨æ•°é‡**: {len(db_info['table_sizes'])} ä¸ª

### ä¸»è¦æ•°æ®è¡¨å¤§å°
"""
        
        # æ·»åŠ è¡¨å¤§å°ä¿¡æ¯
        for schema, table, size, size_bytes, table_size, index_size in db_info['table_sizes'][:10]:
            record_count = db_info['record_counts'].get(table, 'N/A')
            report += f"- **{table}**: {size} ({record_count:,} æ¡è®°å½•)\n"
        
        if backup_info:
            report += f"""
## ğŸ“¦ å¤‡ä»½æ–‡ä»¶å¤§å°åˆ†æ

### å¤‡ä»½æ–‡ä»¶è§„æ ¼
- **åŸå§‹SQLæ–‡ä»¶**: {backup_info['original_size_mb']:.2f} MB
- **å‹ç¼©åå¤§å°**: {backup_info['compressed_size_mb']:.2f} MB
- **å‹ç¼©æ¯”**: {backup_info['compression_ratio']:.1%}

## ğŸ“§ é‚®ä»¶å¤‡ä»½å¯è¡Œæ€§åˆ†æ

### å„é‚®ä»¶æœåŠ¡æ”¯æŒæƒ…å†µ
"""
            
            for service, info in analysis['email_feasible'].items():
                status = "âœ… æ”¯æŒ" if info['feasible'] else "âŒ è¶…é™"
                report += f"- **{service.title()}** (é™åˆ¶{info['limit_mb']}MB): {status} (å ç”¨{info['size_ratio']:.1%})\n"
            
            report += f"""
### ğŸ“‹ å¤‡ä»½ç­–ç•¥å»ºè®®

"""
            for recommendation in analysis['recommendations']:
                report += f"{recommendation}\n"
        
        report += f"""
## â° é‚®ä»¶å¤‡ä»½å‘é€é€»è¾‘

### å½“å‰é…ç½®
- **å¤‡ä»½æ—¶é—´**: æ¯å¤©å‡Œæ™¨ 00:00 (å¯é…ç½®)
- **å¢é‡å¤‡ä»½**: æ¯6å°æ—¶æ‰§è¡Œä¸€æ¬¡
- **é‚®ä»¶å‘é€**: å¤‡ä»½å®Œæˆåç«‹å³å‘é€
- **æ¥æ”¶é‚®ç®±**: James.ni@evertacsolutions.com, james98980566@gmail.com

### å‘é€æµç¨‹
1. **åˆ›å»ºå¤‡ä»½**: ä½¿ç”¨pg_dumpç”ŸæˆSQLæ–‡ä»¶
2. **æ–‡ä»¶å‹ç¼©**: ä½¿ç”¨gzipå‹ç¼©å‡å°‘å¤§å°
3. **å¤§å°æ£€æŸ¥**: éªŒè¯æ˜¯å¦è¶…è¿‡é‚®ä»¶é™„ä»¶é™åˆ¶
4. **é‚®ä»¶å‘é€**: é€šè¿‡SMTPå‘é€åˆ°æŒ‡å®šé‚®ç®±
5. **æœ¬åœ°æ¸…ç†**: åˆ é™¤ä¸´æ—¶å¤‡ä»½æ–‡ä»¶

### æ—¶é—´å®‰æ’ä¼˜åŒ–å»ºè®®
- **å®Œæ•´å¤‡ä»½**: å‡Œæ™¨2:00 (é¿å¼€ä¸šåŠ¡é«˜å³°)
- **å¢é‡å¤‡ä»½**: æ¯8å°æ—¶ (å‡å°‘é¢‘ç‡)
- **é‚®ä»¶å‘é€**: ä»…å®Œæ•´å¤‡ä»½å‘é€é‚®ä»¶
- **æœ¬åœ°ä¿ç•™**: ä¿ç•™æœ€è¿‘3å¤©çš„å¤‡ä»½æ–‡ä»¶

## ğŸ”„ å¤‡ä»½æ–¹æ¡ˆå¯¹æ¯”

| æ–¹æ¡ˆ | ä¼˜ç‚¹ | ç¼ºç‚¹ | é€‚ç”¨åœºæ™¯ |
|------|------|------|----------|
| ğŸ“§ é‚®ä»¶å¤‡ä»½ | ç®€å•å¯é ã€è‡ªåŠ¨å‘é€ | å¤§å°é™åˆ¶ã€ä¾èµ–é‚®ä»¶æœåŠ¡ | å°å‹æ•°æ®åº“(<20MB) |
| ğŸ“ GitHubå¤‡ä»½ | ç‰ˆæœ¬æ§åˆ¶ã€å…è´¹ | éœ€è¦é…ç½®ã€100MBé™åˆ¶ | ä¸­å‹æ•°æ®åº“(<100MB) |
| â˜ï¸ äº‘å­˜å‚¨å¤‡ä»½ | æ— å¤§å°é™åˆ¶ã€é«˜å¯é  | éœ€è¦ä»˜è´¹ã€é…ç½®å¤æ‚ | å¤§å‹æ•°æ®åº“(>100MB) |
| ğŸ’¾ æœ¬åœ°å¤‡ä»½ | é€Ÿåº¦å¿«ã€æ— é™åˆ¶ | å®¹æ˜“ä¸¢å¤±ã€éœ€è¦æ‰‹åŠ¨ç®¡ç† | å¼€å‘æµ‹è¯•ç¯å¢ƒ |

## ğŸ’¡ æœ€ç»ˆå»ºè®®

åŸºäºå½“å‰æ•°æ®åº“è§„æ¨¡åˆ†æï¼Œæ¨èé‡‡ç”¨**æ··åˆå¤‡ä»½ç­–ç•¥**ï¼š

1. **ä¸»è¦æ–¹æ¡ˆ**: {"é‚®ä»¶å¤‡ä»½" if backup_info and backup_info['compressed_size_mb'] <= 20 else "GitHubå¤‡ä»½"}
2. **å¤‡ç”¨æ–¹æ¡ˆ**: äº‘å­˜å‚¨å¤‡ä»½ï¼ˆé•¿æœŸå½’æ¡£ï¼‰
3. **åº”æ€¥æ–¹æ¡ˆ**: æ‰‹åŠ¨ä¸‹è½½å¤‡ä»½

è¿™æ ·å¯ä»¥ç¡®ä¿æ•°æ®å®‰å…¨çš„åŒæ—¶ï¼Œå…¼é¡¾æˆæœ¬å’Œä¾¿åˆ©æ€§ã€‚
"""
        
        return report

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ” å¼€å§‹åˆ†æäº‘ç«¯æ•°æ®åº“å¤§å°å’Œé‚®ä»¶å¤‡ä»½å¯è¡Œæ€§...")
    print("=" * 60)
    
    analyzer = DatabaseSizeAnalyzer()
    
    # 1. è·å–æ•°æ®åº“å¤§å°ä¿¡æ¯
    print("ğŸ“Š è·å–æ•°æ®åº“å¤§å°ä¿¡æ¯...")
    db_info = analyzer.get_database_size_info()
    
    if not db_info:
        print("âŒ æ— æ³•è·å–æ•°æ®åº“ä¿¡æ¯")
        return
    
    print(f"âœ… æ•°æ®åº“å¤§å°: {db_info['db_size_pretty']} ({db_info['db_size_mb']:.2f} MB)")
    print(f"ğŸ“‹ æ€»è®°å½•æ•°: {db_info['total_records']:,} æ¡")
    
    # 2. åˆ›å»ºæµ‹è¯•å¤‡ä»½
    print("\nğŸ“¦ åˆ›å»ºæµ‹è¯•å¤‡ä»½è¯„ä¼°å®é™…å¤§å°...")
    backup_info = analyzer.create_test_backup()
    
    if backup_info:
        print(f"âœ… åŸå§‹å¤‡ä»½: {backup_info['original_size_mb']:.2f} MB")
        print(f"âœ… å‹ç¼©å¤‡ä»½: {backup_info['compressed_size_mb']:.2f} MB")
        print(f"ğŸ“Š å‹ç¼©æ¯”: {backup_info['compression_ratio']:.1%}")
    else:
        print("âŒ æ— æ³•åˆ›å»ºæµ‹è¯•å¤‡ä»½")
    
    # 3. åˆ†æé‚®ä»¶å¤‡ä»½å¯è¡Œæ€§
    print("\nğŸ“§ åˆ†æé‚®ä»¶å¤‡ä»½å¯è¡Œæ€§...")
    analysis = analyzer.analyze_email_backup_feasibility(db_info, backup_info)
    
    if backup_info:
        compressed_size = backup_info['compressed_size_mb']
        if compressed_size <= 20:
            print("âœ… é‚®ä»¶å¤‡ä»½å®Œå…¨å¯è¡Œï¼")
        elif compressed_size <= 25:
            print("âš ï¸ é‚®ä»¶å¤‡ä»½åŸºæœ¬å¯è¡Œï¼Œéœ€è¦é€‰æ‹©åˆé€‚çš„é‚®ä»¶æœåŠ¡")
        else:
            print("âŒ é‚®ä»¶å¤‡ä»½ä¸å¯è¡Œï¼Œå»ºè®®ä½¿ç”¨å…¶ä»–æ–¹æ¡ˆ")
    
    # 4. ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š
    print("\nğŸ“„ ç”Ÿæˆè¯¦ç»†åˆ†ææŠ¥å‘Š...")
    report = analyzer.generate_backup_strategy_report(db_info, backup_info, analysis)
    
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    report_file = f"DATABASE_BACKUP_ANALYSIS_{timestamp}.md"
    
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write(report)
    
    print(f"âœ… æŠ¥å‘Šå·²ç”Ÿæˆ: {report_file}")
    
    # 5. æ˜¾ç¤ºå…³é”®ç»“è®º
    print("\nğŸ¯ å…³é”®ç»“è®º:")
    for recommendation in analysis['recommendations']:
        print(f"  {recommendation}")

if __name__ == "__main__":
    main() 