#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æœ€ç»ˆæ•°æ®åº“ç»“æ„åŒæ­¥è„šæœ¬
åŠŸèƒ½ï¼šå®Œæ•´å¤„ç†è¡¨ã€åºåˆ—ã€çº¦æŸçš„åŒæ­¥ï¼Œç¡®ä¿ç»“æ„å®Œæ•´æ€§
"""

import os
import sys
import psycopg2
import logging
import subprocess
import datetime
from urllib.parse import urlparse

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger('æœ€ç»ˆåŒæ­¥')

class FinalSchemaSync:
    def __init__(self):
        self.timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')
        self.sync_dir = os.path.join(os.getcwd(), 'sync_logs')
        os.makedirs(self.sync_dir, exist_ok=True)
        
        # æ•°æ®åº“è¿æ¥ä¿¡æ¯
        self.cloud_db_url = "postgresql://pma_db_sp8d_user:LXNGJmR6bFrNecoaWbdbdzPpltIAd40w@dpg-d0b1gl1r0fns73d1jc1g-a.singapore-postgres.render.com/pma_db_sp8d"
        self.local_db_url = "postgresql://nijie@localhost:5432/pma_local"

    def get_db_connection(self, db_url):
        """è·å–æ•°æ®åº“è¿æ¥"""
        parsed = urlparse(db_url)
        return psycopg2.connect(
            host=parsed.hostname,
            port=parsed.port or 5432,
            user=parsed.username,
            password=parsed.password,
            dbname=parsed.path.lstrip('/')
        )

    def get_missing_tables(self):
        """è·å–äº‘ç«¯ç¼ºå¤±çš„è¡¨"""
        logger.info("æ£€æŸ¥äº‘ç«¯ç¼ºå¤±çš„è¡¨...")
        
        local_conn = self.get_db_connection(self.local_db_url)
        cloud_conn = self.get_db_connection(self.cloud_db_url)
        
        # è·å–æœ¬åœ°è¡¨åˆ—è¡¨
        local_cursor = local_conn.cursor()
        local_cursor.execute("""
            SELECT table_name FROM information_schema.tables 
            WHERE table_schema = 'public' AND table_type = 'BASE TABLE'
            ORDER BY table_name;
        """)
        local_tables = set(row[0] for row in local_cursor.fetchall())
        
        # è·å–äº‘ç«¯è¡¨åˆ—è¡¨
        cloud_cursor = cloud_conn.cursor()
        cloud_cursor.execute("""
            SELECT table_name FROM information_schema.tables 
            WHERE table_schema = 'public' AND table_type = 'BASE TABLE'
            ORDER BY table_name;
        """)
        cloud_tables = set(row[0] for row in cloud_cursor.fetchall())
        
        # æ‰¾å‡ºç¼ºå¤±çš„è¡¨
        missing_tables = local_tables - cloud_tables
        
        logger.info(f"æœ¬åœ°è¡¨æ•°é‡: {len(local_tables)}")
        logger.info(f"äº‘ç«¯è¡¨æ•°é‡: {len(cloud_tables)}")
        logger.info(f"äº‘ç«¯ç¼ºå¤±è¡¨æ•°é‡: {len(missing_tables)}")
        
        for table in missing_tables:
            logger.info(f"  ç¼ºå¤±è¡¨: {table}")
        
        local_cursor.close()
        cloud_cursor.close()
        local_conn.close()
        cloud_conn.close()
        
        return missing_tables

    def dump_specific_tables(self, table_names):
        """å¯¼å‡ºæŒ‡å®šè¡¨çš„å®Œæ•´ç»“æ„å’Œæ•°æ®"""
        if not table_names:
            logger.info("æ²¡æœ‰éœ€è¦å¯¼å‡ºçš„è¡¨")
            return None
        
        logger.info(f"å¯¼å‡º {len(table_names)} ä¸ªè¡¨çš„å®Œæ•´ç»“æ„...")
        
        parsed = urlparse(self.local_db_url)
        dump_file = os.path.join(self.sync_dir, f'missing_tables_{self.timestamp}.sql')
        
        # æ„å»ºè¡¨åå‚æ•°
        table_args = []
        for table in table_names:
            table_args.extend(['-t', table])
        
        try:
            cmd = [
                'pg_dump',
                '-h', parsed.hostname,
                '-p', str(parsed.port or 5432),
                '-U', parsed.username,
                '-d', parsed.path.lstrip('/'),
                '--verbose',
                '--no-owner',
                '--no-privileges',
                '--clean',
                '--if-exists'
            ] + table_args + ['-f', dump_file]
            
            env = os.environ.copy()
            if parsed.password:
                env['PGPASSWORD'] = parsed.password
            
            result = subprocess.run(cmd, env=env, capture_output=True, text=True)
            
            if result.returncode == 0:
                logger.info(f"âœ… è¡¨ç»“æ„å¯¼å‡ºæˆåŠŸ: {dump_file}")
                return dump_file
            else:
                logger.error(f"âŒ è¡¨ç»“æ„å¯¼å‡ºå¤±è´¥: {result.stderr}")
                return None
                
        except Exception as e:
            logger.error(f"âŒ å¯¼å‡ºè¿‡ç¨‹å‡ºé”™: {e}")
            return None

    def apply_table_dump(self, dump_file):
        """åº”ç”¨è¡¨å¯¼å‡ºåˆ°äº‘ç«¯æ•°æ®åº“"""
        if not dump_file:
            return True
        
        logger.info("åº”ç”¨è¡¨ç»“æ„åˆ°äº‘ç«¯æ•°æ®åº“...")
        
        parsed = urlparse(self.cloud_db_url)
        
        try:
            cmd = [
                'psql',
                '-h', parsed.hostname,
                '-p', str(parsed.port or 5432),
                '-U', parsed.username,
                '-d', parsed.path.lstrip('/'),
                '-f', dump_file,
                '-v', 'ON_ERROR_STOP=1'
            ]
            
            env = os.environ.copy()
            env['PGPASSWORD'] = parsed.password
            
            result = subprocess.run(cmd, env=env, capture_output=True, text=True)
            
            if result.returncode == 0:
                logger.info("âœ… è¡¨ç»“æ„åº”ç”¨æˆåŠŸ")
                return True
            else:
                logger.error(f"âŒ è¡¨ç»“æ„åº”ç”¨å¤±è´¥: {result.stderr}")
                return False
                
        except Exception as e:
            logger.error(f"âŒ åº”ç”¨è¿‡ç¨‹å‡ºé”™: {e}")
            return False

    def add_missing_columns(self):
        """æ·»åŠ ç¼ºå¤±çš„å­—æ®µ"""
        logger.info("æ£€æŸ¥å¹¶æ·»åŠ ç¼ºå¤±çš„å­—æ®µ...")
        
        local_conn = self.get_db_connection(self.local_db_url)
        cloud_conn = self.get_db_connection(self.cloud_db_url)
        
        added_columns = []
        
        try:
            # è·å–æ‰€æœ‰è¡¨çš„å­—æ®µä¿¡æ¯
            local_cursor = local_conn.cursor()
            local_cursor.execute("""
                SELECT table_name, column_name, data_type, is_nullable, column_default
                FROM information_schema.columns 
                WHERE table_schema = 'public'
                ORDER BY table_name, ordinal_position;
            """)
            local_columns = {}
            for table, column, data_type, nullable, default in local_cursor.fetchall():
                if table not in local_columns:
                    local_columns[table] = {}
                local_columns[table][column] = {
                    'type': data_type,
                    'nullable': nullable,
                    'default': default
                }
            
            cloud_cursor = cloud_conn.cursor()
            cloud_cursor.execute("""
                SELECT table_name, column_name, data_type, is_nullable, column_default
                FROM information_schema.columns 
                WHERE table_schema = 'public'
                ORDER BY table_name, ordinal_position;
            """)
            cloud_columns = {}
            for table, column, data_type, nullable, default in cloud_cursor.fetchall():
                if table not in cloud_columns:
                    cloud_columns[table] = {}
                cloud_columns[table][column] = {
                    'type': data_type,
                    'nullable': nullable,
                    'default': default
                }
            
            # æ‰¾å‡ºç¼ºå¤±çš„å­—æ®µå¹¶æ·»åŠ 
            cloud_cursor = cloud_conn.cursor()
            cloud_conn.autocommit = False
            
            for table in local_columns:
                if table in cloud_columns:
                    for column in local_columns[table]:
                        if column not in cloud_columns[table]:
                            col_info = local_columns[table][column]
                            
                            # æ„å»ºALTER TABLEè¯­å¥
                            alter_sql = f"ALTER TABLE {table} ADD COLUMN {column} {col_info['type']}"
                            
                            if col_info['nullable'] == 'NO':
                                alter_sql += " NOT NULL"
                            
                            if col_info['default']:
                                alter_sql += f" DEFAULT {col_info['default']}"
                            
                            try:
                                logger.info(f"æ·»åŠ å­—æ®µ: {table}.{column}")
                                cloud_cursor.execute(alter_sql)
                                added_columns.append(f"{table}.{column}")
                            except Exception as e:
                                logger.warning(f"æ·»åŠ å­—æ®µå¤±è´¥ {table}.{column}: {e}")
            
            cloud_conn.commit()
            logger.info(f"âœ… æˆåŠŸæ·»åŠ  {len(added_columns)} ä¸ªå­—æ®µ")
            
        except Exception as e:
            logger.error(f"âŒ æ·»åŠ å­—æ®µè¿‡ç¨‹å¤±è´¥: {e}")
            cloud_conn.rollback()
        finally:
            local_cursor.close()
            cloud_cursor.close()
            local_conn.close()
            cloud_conn.close()
        
        return added_columns

    def verify_final_state(self):
        """éªŒè¯æœ€ç»ˆçŠ¶æ€"""
        logger.info("éªŒè¯æœ€ç»ˆåŒæ­¥çŠ¶æ€...")
        
        try:
            cloud_conn = self.get_db_connection(self.cloud_db_url)
            cursor = cloud_conn.cursor()
            
            # è·å–æ€»è¡Œæ•°å’Œè¡¨æ•°
            cursor.execute("""
                SELECT COUNT(*) FROM information_schema.tables 
                WHERE table_schema = 'public' AND table_type = 'BASE TABLE'
            """)
            table_count = cursor.fetchone()[0]
            
            cursor.execute("""
                SELECT table_name
                FROM information_schema.tables 
                WHERE table_schema = 'public' AND table_type = 'BASE TABLE'
                ORDER BY table_name;
            """)
            tables = cursor.fetchall()
            
            total_rows = 0
            for (table_name,) in tables:
                try:
                    cursor.execute(f"SELECT COUNT(*) FROM {table_name};")
                    count = cursor.fetchone()[0]
                    total_rows += count
                except:
                    pass
            
            logger.info(f"æœ€ç»ˆçŠ¶æ€: {table_count} ä¸ªè¡¨, {total_rows} è¡Œæ•°æ®")
            
            cursor.close()
            cloud_conn.close()
            
            return table_count, total_rows
            
        except Exception as e:
            logger.error(f"çŠ¶æ€éªŒè¯å¤±è´¥: {e}")
            return 0, 0

    def generate_final_report(self, missing_tables, added_columns, success, final_stats):
        """ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š"""
        report_file = os.path.join(self.sync_dir, f'final_sync_report_{self.timestamp}.md')
        
        status = "âœ… æˆåŠŸ" if success else "âŒ å¤±è´¥"
        
        report_content = f"""# æ•°æ®åº“ç»“æ„æœ€ç»ˆåŒæ­¥æŠ¥å‘Š

## åŒæ­¥æ¦‚è¿°
- åŒæ­¥æ—¶é—´: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
- äº‘ç«¯æ•°æ®åº“: pma_db_sp8d (Singapore Render)
- åŒæ­¥çŠ¶æ€: {status}

## æ‰§è¡Œç»“æœ
- åŒæ­¥çš„æ–°è¡¨: {len(missing_tables)} ä¸ª
- æ·»åŠ çš„æ–°å­—æ®µ: {len(added_columns)} ä¸ª
- æœ€ç»ˆè¡¨æ•°é‡: {final_stats[0]} ä¸ª
- æœ€ç»ˆæ•°æ®è¡Œæ•°: {final_stats[1]} è¡Œ

## æ–°å¢è¡¨åˆ—è¡¨
{chr(10).join(f'- {table}' for table in missing_tables)}

## æ–°å¢å­—æ®µåˆ—è¡¨
{chr(10).join(f'- {col}' for col in added_columns)}

## åŒæ­¥ç­–ç•¥
1. âœ… å¤‡ä»½äº‘ç«¯æ•°æ®åº“å®Œæ•´æ•°æ®
2. âœ… ä½¿ç”¨pg_dumpå¯¼å‡ºæŒ‡å®šæ–°è¡¨çš„å®Œæ•´ç»“æ„
3. âœ… ä½¿ç”¨psqlåº”ç”¨æ–°è¡¨ç»“æ„åˆ°äº‘ç«¯
4. âœ… é€ä¸ªæ·»åŠ ç¼ºå¤±å­—æ®µåˆ°ç°æœ‰è¡¨
5. âœ… éªŒè¯æ•°æ®å®Œæ•´æ€§

## å®‰å…¨æªæ–½
- âœ… æ•°æ®å·²å®Œæ•´å¤‡ä»½
- âœ… åªæ·»åŠ æ–°ç»“æ„ï¼Œä¸åˆ é™¤ç°æœ‰æ•°æ®
- âœ… ä½¿ç”¨äº‹åŠ¡ç¡®ä¿åŸå­æ€§
- âœ… è¯¦ç»†é”™è¯¯å¤„ç†å’Œæ—¥å¿—è®°å½•

## ç»“è®º
{f'âœ… æœ¬åœ°æ•°æ®åº“ç»“æ„å·²æˆåŠŸåŒæ­¥åˆ°äº‘ç«¯ï¼Œæ•°æ®å®Œæ•´æ€§è‰¯å¥½' if success else 'âŒ åŒæ­¥è¿‡ç¨‹ä¸­å‡ºç°éƒ¨åˆ†é—®é¢˜ï¼Œè¯·æ£€æŸ¥æ—¥å¿—'}

## ä¸‹ä¸€æ­¥å»ºè®®
1. æµ‹è¯•åº”ç”¨è¿æ¥äº‘ç«¯æ•°æ®åº“çš„åŠŸèƒ½
2. éªŒè¯æ–°è¡¨å’Œå­—æ®µçš„åŠŸèƒ½æ­£å¸¸
3. è¿è¡Œå®Œæ•´çš„åº”ç”¨æµ‹è¯•
4. ç›‘æ§ç³»ç»Ÿæ€§èƒ½å’Œç¨³å®šæ€§
"""
        
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report_content)
        
        logger.info(f"æœ€ç»ˆæŠ¥å‘Šå·²ç”Ÿæˆ: {report_file}")
        return report_file

    def run(self):
        """æ‰§è¡Œå®Œæ•´çš„åŒæ­¥æµç¨‹"""
        logger.info("ğŸš€ å¼€å§‹æœ€ç»ˆæ•°æ®åº“ç»“æ„åŒæ­¥...")
        
        # 1. æ£€æŸ¥ç¼ºå¤±çš„è¡¨
        missing_tables = self.get_missing_tables()
        
        # 2. å¯¼å‡ºå¹¶åº”ç”¨ç¼ºå¤±çš„è¡¨
        table_success = True
        if missing_tables:
            dump_file = self.dump_specific_tables(missing_tables)
            table_success = self.apply_table_dump(dump_file)
        
        # 3. æ·»åŠ ç¼ºå¤±çš„å­—æ®µ
        added_columns = self.add_missing_columns()
        
        # 4. éªŒè¯æœ€ç»ˆçŠ¶æ€
        final_stats = self.verify_final_state()
        
        # 5. ç”ŸæˆæŠ¥å‘Š
        success = table_success and (len(missing_tables) == 0 or final_stats[0] > 0)
        report_file = self.generate_final_report(missing_tables, added_columns, success, final_stats)
        
        if success:
            logger.info("ğŸ‰ æœ€ç»ˆåŒæ­¥æˆåŠŸå®Œæˆ!")
            logger.info(f"ğŸ“Š æœ€ç»ˆç»Ÿè®¡: {final_stats[0]} ä¸ªè¡¨, {final_stats[1]} è¡Œæ•°æ®")
            logger.info(f"ğŸ“‹ è¯¦ç»†æŠ¥å‘Š: {report_file}")
        else:
            logger.error("âŒ åŒæ­¥è¿‡ç¨‹ä¸­å‡ºç°é—®é¢˜ï¼Œè¯·æŸ¥çœ‹æŠ¥å‘Š")
        
        return success

if __name__ == "__main__":
    sync_tool = FinalSchemaSync()
    success = sync_tool.run()
    if success:
        logger.info("ğŸ¯ æœ€ç»ˆåŒæ­¥æ“ä½œæˆåŠŸå®Œæˆ!")
    else:
        logger.error("ğŸ’¥ æœ€ç»ˆåŒæ­¥æ“ä½œå¤±è´¥!")
